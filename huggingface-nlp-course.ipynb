{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bad813e3-69ce-45e8-b4ee-dfd6eec7873f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Huggingface - NLP Crourse - Chapter 1: Transformer Models\n",
    "\n",
    "I'll be following along with: https://huggingface.co/learn/nlp-course/chapter1/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca8c1e08-9ad0-4c20-8e00-83d8cd14bf5c",
   "metadata": {},
   "source": [
    "## Demo of `pipeline()` capabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ac46acd3-a7ea-4608-843e-d177d6031533",
   "metadata": {},
   "outputs": [],
   "source": [
    "# requirements\n",
    "# ---\n",
    "# !pip install transformers[sentencepiece]\n",
    "# !pip install sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a740f6b3-b653-4b1f-9011-c92837ed057a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f271817c-7c6a-4838-9cb4-9545fc9680b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert-base-uncased-finetuned-sst-2-english and revision af0f99b (https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'label': 'POSITIVE', 'score': 0.9598049521446228},\n",
       " {'label': 'NEGATIVE', 'score': 0.9994558691978455}]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier = pipeline(\"sentiment-analysis\")\n",
    "classifier([\n",
    "  \"I've been waiting for a HuggingFace course my whole life.\",\n",
    "  \"I hate this so much!\"\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "00fb0cbb-f1f0-4278-a1c8-fca492b0af0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to facebook/bart-large-mnli and revision c626438 (https://huggingface.co/facebook/bart-large-mnli).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'sequence': 'This is a course about the Transformers library',\n",
       " 'labels': ['education', 'business', 'politics'],\n",
       " 'scores': [0.8445982336997986, 0.1119748130440712, 0.043426983058452606]}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier = pipeline(\"zero-shot-classification\")\n",
    "classifier(\n",
    "    \"This is a course about the Transformers library\",\n",
    "    candidate_labels=[\"education\", \"politics\", \"business\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "496c7886-37e1-43d6-80e9-16556558fedb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to gpt2 and revision 6c0e608 (https://huggingface.co/gpt2).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'In this course, we will teach you how to develop a fully automated production model using an open source software framework, which comes with all the tools needed to efficiently work with your environment.\\n\\nI will show you two ways you can configure how you'}]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generator = pipeline(\"text-generation\")\n",
    "generator(\"In this course, we will teach you how to\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0c5a5e0a-868c-422f-9d0d-3c337eb1f2ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'In this course, we will teach you how to use an HTML5-based app on HTML5-powered tablets. In this course, we will'},\n",
       " {'generated_text': 'In this course, we will teach you how to start DGX.'}]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generator = pipeline(\"text-generation\", model=\"distilgpt2\")\n",
    "generator(\n",
    "    \"In this course, we will teach you how to\",\n",
    "    max_length=30,\n",
    "    num_return_sequences=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "18e184e6-8b7a-44e5-accd-e2eb095130fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilroberta-base and revision ec58a5b (https://huggingface.co/distilroberta-base).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Some weights of the model checkpoint at distilroberta-base were not used when initializing RobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'score': 0.1961977779865265,\n",
       "  'token': 30412,\n",
       "  'token_str': ' mathematical',\n",
       "  'sequence': 'This course will teach you all about mathematical models.'},\n",
       " {'score': 0.0405273362994194,\n",
       "  'token': 38163,\n",
       "  'token_str': ' computational',\n",
       "  'sequence': 'This course will teach you all about computational models.'}]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unmasker = pipeline(\"fill-mask\")\n",
    "unmasker(\"This course will teach you all about <mask> models.\", top_k=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2aaa94fe-ad62-45f4-953f-aec916266d8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to dbmdz/bert-large-cased-finetuned-conll03-english and revision f2482bf (https://huggingface.co/dbmdz/bert-large-cased-finetuned-conll03-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Some weights of the model checkpoint at dbmdz/bert-large-cased-finetuned-conll03-english were not used when initializing BertForTokenClassification: ['bert.pooler.dense.weight', 'bert.pooler.dense.bias']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "C:\\Users\\p\\Desktop\\_ML\\huggingface-nlp-course\\venv\\Lib\\site-packages\\transformers\\pipelines\\token_classification.py:169: UserWarning: `grouped_entities` is deprecated and will be removed in version v5.0.0, defaulted to `aggregation_strategy=\"AggregationStrategy.SIMPLE\"` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'entity_group': 'PER',\n",
       "  'score': 0.9981694,\n",
       "  'word': 'Sylvain',\n",
       "  'start': 11,\n",
       "  'end': 18},\n",
       " {'entity_group': 'ORG',\n",
       "  'score': 0.9796019,\n",
       "  'word': 'Hugging Face',\n",
       "  'start': 33,\n",
       "  'end': 45},\n",
       " {'entity_group': 'LOC',\n",
       "  'score': 0.9932106,\n",
       "  'word': 'Brooklyn',\n",
       "  'start': 49,\n",
       "  'end': 57}]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ner = pipeline(\"ner\", grouped_entities=True)\n",
    "ner(\"My name is Sylvain and I work at Hugging Face in Brooklyn.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0677841e-1b1f-40fd-bbd5-9863998517ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert-base-cased-distilled-squad and revision 626af31 (https://huggingface.co/distilbert-base-cased-distilled-squad).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'score': 0.6949770450592041, 'start': 33, 'end': 45, 'answer': 'Hugging Face'}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question_answerer = pipeline(\"question-answering\")\n",
    "question_answerer(\n",
    "    question=\"Where do I work?\",\n",
    "    context=\"My name is Sylvain and I work at Hugging Face in Brooklyn\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8dc08ed2-720f-44b4-a8c3-26cf2175c548",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to sshleifer/distilbart-cnn-12-6 and revision a4f8f3e (https://huggingface.co/sshleifer/distilbart-cnn-12-6).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'summary_text': ' America has changed dramatically during recent years . The number of engineering graduates in the U.S. has declined in traditional engineering disciplines such as mechanical, civil,    electrical, chemical, and aeronautical engineering . Rapidly developing economies such as China and India continue to encourage and advance the teaching of engineering .'}]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summarizer = pipeline(\"summarization\")\n",
    "summarizer(\n",
    "    \"\"\"\n",
    "    America has changed dramatically during recent years. Not only has the number of \n",
    "    graduates in traditional engineering disciplines such as mechanical, civil, \n",
    "    electrical, chemical, and aeronautical engineering declined, but in most of \n",
    "    the premier American universities engineering curricula now concentrate on \n",
    "    and encourage largely the study of engineering science. As a result, there \n",
    "    are declining offerings in engineering subjects dealing with infrastructure, \n",
    "    the environment, and related issues, and greater concentration on high \n",
    "    technology subjects, largely supporting increasingly complex scientific \n",
    "    developments. While the latter is important, it should not be at the expense \n",
    "    of more traditional engineering.\n",
    "\n",
    "    Rapidly developing economies such as China and India, as well as other \n",
    "    industrial countries in Europe and Asia, continue to encourage and advance \n",
    "    the teaching of engineering. Both China and India, respectively, graduate \n",
    "    six and eight times as many traditional engineers as does the United States. \n",
    "    Other industrial countries at minimum maintain their output, while America \n",
    "    suffers an increasingly serious decline in the number of engineering graduates \n",
    "    and a lack of well-educated engineers.\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9fd924e8-54f4-47b9-8f68-c0b2146b515f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\p\\Desktop\\_ML\\huggingface-nlp-course\\venv\\Lib\\site-packages\\transformers\\models\\marian\\tokenization_marian.py:194: UserWarning: Recommended: pip install sacremoses.\n",
      "  warnings.warn(\"Recommended: pip install sacremoses.\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'translation_text': 'This course is produced by Hugging Face.'}]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translator = pipeline(\"translation\", model=\"Helsinki-NLP/opus-mt-fr-en\")\n",
    "translator(\"Ce cours est produit par Hugging Face.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d24630b6-a2f2-4a54-882b-626d961222b5",
   "metadata": {},
   "source": [
    "## Bias and limitations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1d656737-f931-447d-9d2d-4a01118450d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'bert.pooler.dense.weight', 'bert.pooler.dense.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['carpenter', 'lawyer', 'farmer', 'businessman', 'doctor']\n",
      "['nurse', 'maid', 'teacher', 'waitress', 'prostitute']\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "unmasker = pipeline(\"fill-mask\", model=\"bert-base-uncased\")\n",
    "result = unmasker(\"This man works as a [MASK].\")\n",
    "print([r[\"token_str\"] for r in result])\n",
    "\n",
    "result = unmasker(\"This woman works as a [MASK].\")\n",
    "print([r[\"token_str\"] for r in result])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ddcd1e4-888b-41a0-896d-cbe829b1d430",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Huggingface - NLP Crourse - Chapter 2: Using 🤗 Transformers\n",
    "I'll be following along with: https://huggingface.co/learn/nlp-course/chapter2/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fcacff8-6e68-4a39-b01c-4ce2574940e2",
   "metadata": {},
   "source": [
    "## Re-creating the `pipeline()`\n",
    "We are going to replicate the functionality of this `pipeline()` step by step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "afe6966b-ca61-4985-974f-65a1cf3a8a77",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert-base-uncased-finetuned-sst-2-english and revision af0f99b (https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'label': 'POSITIVE', 'score': 0.9598049521446228},\n",
       " {'label': 'NEGATIVE', 'score': 0.9994558691978455}]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "classifier = pipeline(\"sentiment-analysis\")\n",
    "classifier(\n",
    "    [\n",
    "        \"I've been waiting for a HuggingFace course my whole life.\",\n",
    "        \"I hate this so much!\",\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a946538a-3267-44e4-bc9a-1053df204cf2",
   "metadata": {},
   "source": [
    "### 1- Tokenizer\n",
    "`pipeline()` pre-process the inputs into tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a6553044-e121-442f-b81c-aad8387050bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1c9ea5c3-c717-43fd-8512-97711260979f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[  101,  1045,  1005,  2310,  2042,  3403,  2005,  1037, 17662, 12172,\n",
      "          2607,  2026,  2878,  2166,  1012,   102],\n",
      "        [  101,  1045,  5223,  2023,  2061,  2172,   999,   102,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0]])}\n"
     ]
    }
   ],
   "source": [
    "raw_inputs = [\n",
    "    \"I've been waiting for a HuggingFace course my whole life.\",\n",
    "    \"I hate this so much!\",\n",
    "]\n",
    "inputs = tokenizer(raw_inputs, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "print(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0725f065-b476-4c87-9a62-8709cbf46301",
   "metadata": {},
   "source": [
    "### 2- Model\n",
    "The tokens are passed to a pre-trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9043d28b-2909-4192-9f1f-b595bab7289d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel\n",
    "\n",
    "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "model = AutoModel.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b7e68517-276c-40ca-866d-037b920f6496",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 16, 768])\n"
     ]
    }
   ],
   "source": [
    "outputs = model(**inputs)\n",
    "print(outputs.last_hidden_state.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd179a86-493f-4983-a72b-7e9ba3919a15",
   "metadata": {},
   "source": [
    "It produces a tensor `torch.Size([2, 16, 768])` or `batch_size = 2`, `sequence_length = 16`, and `hidden_size = 768` because the `AutoModel` doesn't include the head of the model.\n",
    "\n",
    "To have the output of the head we use `AutoModelForSequenceClassification` instead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "15cd0093-5f91-4217-806e-c3b32c677617",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
    "outputs = model(**inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fafc370a-caf2-440f-b595-d808d00c5b48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.5607,  1.6123],\n",
      "        [ 4.1692, -3.3464]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(outputs.logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6732d817-18dd-453e-b0cb-c56ce2c81649",
   "metadata": {},
   "source": [
    "The output tensor of size [2, 2] is not yet probabilities (they do not sum to 1), because the model returns logits."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eccc2c34-5b90-461b-af6f-87ed6ab2067f",
   "metadata": {},
   "source": [
    "### 3- Postprocessing\n",
    "Apply `softmax()` to logits to normalize the probabilities to 1 and retrieve the labels in `model.config.id2label`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "52af8b9d-6d83-4180-bbae-2b52c4a8e779",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[4.0195e-02, 9.5980e-01],\n",
      "        [9.9946e-01, 5.4418e-04]], grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "43142a2e-0b25-4456-a55a-ee79aed895c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'NEGATIVE', 1: 'POSITIVE'}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.config.id2label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d3087c56-41aa-4e13-ab2c-43a8ecde3cf8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['NEGATIVE : 4.02%', 'POSITIVE : 95.98%'],\n",
       " ['NEGATIVE : 99.95%', 'POSITIVE : 0.05%']]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[[f'{model.config.id2label[i]} : {pred:.2%}' for i, pred in enumerate(prediction)] for prediction in predictions]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52c692c5-338b-4e66-b6a7-e22e0d4c3940",
   "metadata": {},
   "source": [
    "## Model and a checkpoint\n",
    "Instantiating a model with no checkpoint makes a model with random weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "016b7ec8-e20c-4290-a5c4-97228e5fae15",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertConfig, BertModel\n",
    "\n",
    "# Building the config\n",
    "config = BertConfig()\n",
    "\n",
    "# Building the model from the config\n",
    "model = BertModel(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "411a2e98-18c0-41c0-b020-b2eaa1f89413",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.32.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6857774-dc0c-4d0a-b7fc-32d181639572",
   "metadata": {},
   "source": [
    "Let's load a pre-trained model from a checkpoint instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "72aa557e-b39f-4a6b-967b-b3bcff678465",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertModel\n",
    "\n",
    "model = BertModel.from_pretrained(\"bert-base-cased\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44a62c61-c5d8-4f08-a369-b8eaf26d99c9",
   "metadata": {},
   "source": [
    "Models can be saved to disk using `.save_pretrained()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d1ac51d0-c44a-403a-8fea-a3ae7f0c934e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(\"directory_on_my_computer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6970ac13-f144-4113-80c6-1cd94855f6d3",
   "metadata": {},
   "source": [
    "## Tokenizer\n",
    "Models can only work with number, so inputs are converted to numbers before being fed to the model. The 3 main tokenization algorithms are:\n",
    "- Word based\n",
    "- Character based\n",
    "- Subword based"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fc9e732-af9d-42fc-86eb-56ec34d007fd",
   "metadata": {},
   "source": [
    "### Word based tokenization\n",
    "Split text into words (by splitting on whitespace) and words are then converted to a unique id.\n",
    "\n",
    "e.g.: `'I like turtle'` -> `['I', 'like', 'turtle']` -> `[17, 494, 237]`\n",
    "\n",
    "Each word carry a lot of context and semantic meaning, but it has limitations, `dog` is almost like `dogs` but they will be treated completely differently. We also need an id for each word in the vocabulary which create a very large mapping and makes models bloated. Every new word will be treated as `out_of_vocabulary` which will also lead to a loss of information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c826a099-96b4-4324-9d08-84a0c2d880d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'like', 'turtle']\n",
      "[17, 494, 237]\n"
     ]
    }
   ],
   "source": [
    "vocabulary = {\n",
    "  '[OUT OF VOCABULARY]': 0,\n",
    "  'I': 17,\n",
    "  'turtle': 237,\n",
    "  'like': 494,\n",
    "}\n",
    "text = 'I like turtle'\n",
    "tokens = text.split()\n",
    "print(tokens)\n",
    "numerical_tokens = [vocabulary.get(t, 0) for t in tokens]\n",
    "print(numerical_tokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9e53d93-3d0d-4f64-a8b3-cda40f67e70d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Character based tokenization\n",
    "Split text into characters.\n",
    "\n",
    "e.g.: `'I like turtle'` -> `['I', ' ', 'l', 'i', 'k', 'e', ' ', 't', 'u', 'r', 't', 'l', 'e']` -> `[73, 32, 108, 105, 107, 101, 32, 116, 117, 114, 116, 108, 101]`\n",
    "\n",
    "The vocabulary is much smaller and cover the entire possible words spelled in your charset. But each token has a weaker meaning and hold less information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "119bd3f6-6941-45bf-842b-fb82e1f584e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', ' ', 'l', 'i', 'k', 'e', ' ', 't', 'u', 'r', 't', 'l', 'e']\n",
      "[73, 32, 108, 105, 107, 101, 32, 116, 117, 114, 116, 108, 101]\n"
     ]
    }
   ],
   "source": [
    "vocabulary = {c: i for i, c in enumerate([chr(n) for n in range(255)])}\n",
    "text = 'I like turtle'\n",
    "tokens = list(text)\n",
    "print(tokens)\n",
    "numerical_tokens = [vocabulary.get(t, 0) for t in tokens]\n",
    "print(numerical_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58018189-533d-4933-8f03-3e08f6f25152",
   "metadata": {},
   "source": [
    "## Subword based tokenization\n",
    "It's a compromise between word and char tokenizer.\n",
    "\n",
    "Frequent words are not split, while rare words are decomposed into meaningful subwords.\n",
    "\n",
    "e.g.:\n",
    "- `dog` -> `dog\\w`\n",
    "- `dogs` -> [`dog`, `s\\w`]\n",
    "- `tokenization` -> [`token`, `##ization`]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3b57b797-138e-4354-bbf3-9b0c06362bb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-cased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f24c90e7-5c72-4baf-a38a-130286a07b64",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "12104d3d-d44d-477d-85b1-ee5cd6b47c45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Using', 'a', 'Trans', '##former', 'network', 'is', 'simple']\n",
      "[7993, 170, 13809, 23763, 2443, 1110, 3014]\n",
      "Using a transformer network is simple\n"
     ]
    }
   ],
   "source": [
    "sequence = \"Using a Transformer network is simple\"\n",
    "\n",
    "# tokenize step by step\n",
    "tokens = tokenizer.tokenize(sequence)\n",
    "print(tokens)\n",
    "ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "print(ids)\n",
    "\n",
    "# or use the equivalent\n",
    "tokenizer(sequence)\n",
    "\n",
    "# decoding\n",
    "decoded_string = tokenizer.decode([7993, 170, 11303, 1200, 2443, 1110, 3014])\n",
    "print(decoded_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3443897-06cd-46e2-8972-ae185e34cf49",
   "metadata": {},
   "source": [
    "## Tokenizing multiple inputs at once\n",
    "When tokenizing multiple inputs we have to make sure they are the same size as tensors need to have regular dimensions. So the inputs are padded up to a certain size. To match that we pass a mask for the attention layer so the padding tokens are ignored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "70657951-e3d1-4adc-afa9-9fffe0c91d73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input IDs: tensor([[ 1045,  1005,  2310,  2042,  3403,  2005,  1037, 17662, 12172,  2607,\n",
      "          2026,  2878,  2166,  1012]])\n",
      "Logits: tensor([[-2.7276,  2.8789]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
    "\n",
    "sequence = \"I've been waiting for a HuggingFace course my whole life.\"\n",
    "\n",
    "tokens = tokenizer.tokenize(sequence)\n",
    "ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "\n",
    "input_ids = torch.tensor([ids])\n",
    "print(\"Input IDs:\", input_ids)\n",
    "\n",
    "output = model(input_ids)\n",
    "print(\"Logits:\", output.logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3443c26-6add-4907-ab6b-21ad69b94108",
   "metadata": {},
   "source": [
    "For batched requests we need to pad the inputs to be the same size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "87ac0dfb-a901-4286-ad75-e1fe9c648564",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.5694, -1.3895],\n",
      "        [ 0.5803, -0.4125]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "batched_ids = [\n",
    "    [200, 200, 200],\n",
    "    [200, 200, tokenizer.pad_token_id],\n",
    "]\n",
    "\n",
    "attention_mask = [\n",
    "    [1, 1, 1],\n",
    "    [1, 1, 0],\n",
    "]\n",
    "\n",
    "outputs = model(torch.tensor(batched_ids), attention_mask=torch.tensor(attention_mask))\n",
    "print(outputs.logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ed19669-7901-400a-8989-6b0aa31c70c7",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Huggingface - NLP Crourse - Chapter 3: Fine-Tuning a Pretrained Model\n",
    "I'll be following along with: https://huggingface.co/learn/nlp-course/chapter3/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d34f1399-2664-4001-a4ed-d14ea24dff90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install datasets evaluate transformers[torch] scipy sklearn scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34f56ada-e4c5-4e4c-96ac-cab45a9987c0",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "The dataset is stored on disk using [Apache Arrow](https://arrow.apache.org/docs/python/dataset.html). Only requested rows are loaded in memory. So we can manipulate big datasets without going OOM."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91c063e0-c18a-4b24-bff5-698990b28910",
   "metadata": {},
   "source": [
    "## Load the dataset\n",
    "Here we are using a dataset comming from a [Microsoft paper](https://aclanthology.org/I05-5002.pdf), it provides a corpus for \"Sentential Paraphrases\" (aka. equivalent sentences).\n",
    "\n",
    "Additional datasets can be found at: https://huggingface.co/datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9dd0deca-c749-4dc2-8e92-6779a0a3e9ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['sentence1', 'sentence2', 'label', 'idx'],\n",
       "        num_rows: 3668\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['sentence1', 'sentence2', 'label', 'idx'],\n",
       "        num_rows: 408\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['sentence1', 'sentence2', 'label', 'idx'],\n",
       "        num_rows: 1725\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "raw_datasets = load_dataset(\"glue\", \"mrpc\")\n",
    "raw_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9eee00ff-f349-475d-bc1a-89cd4daed8fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sentence1': 'Amrozi accused his brother , whom he called \" the witness \" , of deliberately distorting his evidence .',\n",
       " 'sentence2': 'Referring to him as only \" the witness \" , Amrozi accused his brother of deliberately distorting his evidence .',\n",
       " 'label': 1,\n",
       " 'idx': 0}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_train_dataset = raw_datasets[\"train\"]\n",
    "raw_train_dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6114601d-9069-480f-9b1d-f2fbc1195688",
   "metadata": {},
   "source": [
    " `.features['label']` provides the correspondance between the label id and the human meaning. Here `id 0 = 'not_equivalent'` and `id 1 = 'equivalent'`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1438b5d2-a8cd-4114-9bcc-17ce00b6f41b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sentence1': Value(dtype='string', id=None),\n",
       " 'sentence2': Value(dtype='string', id=None),\n",
       " 'label': ClassLabel(names=['not_equivalent', 'equivalent'], id=None),\n",
       " 'idx': Value(dtype='int32', id=None)}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_train_dataset.features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f5db3da-2ec5-4886-9a63-609f3189f369",
   "metadata": {},
   "source": [
    "## Tokenize the entries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc725cc9-9514-48a5-83fb-08b941696793",
   "metadata": {},
   "source": [
    "### With fixed padding\n",
    "This is the easiest solution. Pad the entire dataset to a fixed size (here 128) and this is fast on TPU.\n",
    "\n",
    "Note: Dynamic padding can provide a speedup on CPU/GPU for the batches with only smaller entries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "53208af1-318d-4e79-9d27-d3210be42341",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 3668\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 408\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 1725\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "checkpoint = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "\n",
    "def tokenize_function(example):\n",
    "    return tokenizer(example[\"sentence1\"], example[\"sentence2\"], padding='max_length', truncation=True, max_length=128)\n",
    "\n",
    "tokenized_datasets = raw_datasets.map(tokenize_function, batched=True)\n",
    "tokenized_datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62e3e5dd-2f1e-43ff-8784-e51036b6f965",
   "metadata": {},
   "source": [
    "### With dynamic padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "655982c4-c5fd-49d1-9819-1d86985a3d3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, DataCollatorWithPadding\n",
    "\n",
    "checkpoint = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "\n",
    "def tokenize_function(example):\n",
    "    return tokenizer(example[\"sentence1\"], example[\"sentence2\"], truncation=True)\n",
    "\n",
    "tokenized_datasets = raw_datasets.map(tokenize_function, batched=True)\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26ac894d-7591-472d-ad5d-4d887b9ec311",
   "metadata": {},
   "source": [
    "### Cleanup the data\n",
    "Remove unused columns and rename `label` to `labels` to make the huggingface model happy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "11d73e41-9043-43ba-a031-fd14abded5c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['labels', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "    num_rows: 3668\n",
       "})"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_datasets = tokenized_datasets.remove_columns(['idx', 'sentence1', 'sentence2'])\n",
    "tokenized_datasets = tokenized_datasets.rename_column('label', 'labels')\n",
    "tokenized_datasets = tokenized_datasets.with_format('torch')\n",
    "tokenized_datasets['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a494dd23-793f-4103-aee5-95c19abf9628",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: we can get a smaller subset of the data by using `.select()`\n",
    "# small_train_dataset = tokenized_datasets['train'].select(range(100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a3c1d72-adaa-4612-93e9-7cc5d9a9b8bb",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f3ac7d5c-f1a9-4c2c-8ade-d2318d740e77",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "from transformers import Trainer\n",
    "\n",
    "training_args = TrainingArguments(\"test-trainer\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)\n",
    "trainer = Trainer(\n",
    "    model,\n",
    "    training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "97509826-6ce8-4558-a38d-b5abe0155e11",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1377' max='1377' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1377/1377 01:19, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.597400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.476100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1377, training_loss=0.490657354838275, metrics={'train_runtime': 81.1125, 'train_samples_per_second': 135.663, 'train_steps_per_second': 16.976, 'total_flos': 405324636337200.0, 'train_loss': 0.490657354838275, 'epoch': 3.0})"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "442bc352-2874-49be-9d26-4d952d1ac2a9",
   "metadata": {},
   "source": [
    "### Evaluate the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "4e32eeb0-e1c8-4358-b0b1-ad40042eca8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(408, 2) (408,)\n"
     ]
    }
   ],
   "source": [
    "predictions = trainer.predict(tokenized_datasets[\"validation\"])\n",
    "print(predictions.predictions.shape, predictions.label_ids.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "668bba8d-0da9-43aa-990c-d416b05945c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.8308823529411765, 'f1': 0.881646655231561}"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import evaluate\n",
    "\n",
    "preds = np.argmax(predictions.predictions, axis=-1)\n",
    "metric = evaluate.load(\"glue\", \"mrpc\")\n",
    "metric.compute(predictions=preds, references=predictions.label_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30e5b5d9-1e40-43ef-baff-3e7cf9950048",
   "metadata": {},
   "source": [
    "### Train + Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "f6d29e1e-fe2e-4144-96a2-9ddcb8d86f27",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1377' max='1377' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1377/1377 01:20, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.343839</td>\n",
       "      <td>0.852941</td>\n",
       "      <td>0.896552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.492500</td>\n",
       "      <td>0.607140</td>\n",
       "      <td>0.848039</td>\n",
       "      <td>0.897010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.289700</td>\n",
       "      <td>0.715746</td>\n",
       "      <td>0.857843</td>\n",
       "      <td>0.899306</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1377, training_loss=0.32119023773924715, metrics={'train_runtime': 80.515, 'train_samples_per_second': 136.67, 'train_steps_per_second': 17.102, 'total_flos': 405540469624800.0, 'train_loss': 0.32119023773924715, 'epoch': 3.0})"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def compute_metrics(eval_preds):\n",
    "    metric = evaluate.load(\"glue\", \"mrpc\")\n",
    "    logits, labels = eval_preds\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels)\n",
    "\n",
    "training_args = TrainingArguments(\"test-trainer\", evaluation_strategy=\"epoch\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model,\n",
    "    training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98c7a7aa-29e3-4da1-93bb-0d6a7e8b9720",
   "metadata": {},
   "source": [
    "## Train by hand with PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "3664d1e9-4f14-4217-bf7f-20aff7a840fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\p\\Desktop\\_ML\\huggingface-nlp-course\\venv\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65077a413fdf48f98b8ad74b5ce06f67",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1377 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from accelerate import Accelerator\n",
    "from transformers import AdamW, AutoModelForSequenceClassification, get_scheduler\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    tokenized_datasets[\"train\"], shuffle=True, batch_size=8, collate_fn=data_collator\n",
    ")\n",
    "eval_dataloader = DataLoader(\n",
    "    tokenized_datasets[\"validation\"], batch_size=8, collate_fn=data_collator\n",
    ")\n",
    "\n",
    "accelerator = Accelerator()\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)\n",
    "optimizer = AdamW(model.parameters(), lr=3e-5)\n",
    "\n",
    "train_dl, eval_dl, model, optimizer = accelerator.prepare(\n",
    "    train_dataloader, eval_dataloader, model, optimizer\n",
    ")\n",
    "\n",
    "num_epochs = 3\n",
    "num_training_steps = num_epochs * len(train_dl)\n",
    "lr_scheduler = get_scheduler(\n",
    "    \"linear\",\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=num_training_steps,\n",
    ")\n",
    "\n",
    "progress_bar = tqdm(range(num_training_steps))\n",
    "\n",
    "model.train()\n",
    "for epoch in range(num_epochs):\n",
    "    for batch in train_dl:\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        accelerator.backward(loss)\n",
    "\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "        progress_bar.update(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13171cd1-a386-419c-b26e-e89e4b0bc965",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Huggingface - NLP Crourse - Chapter 4: Sharing Models and Tokenizers\n",
    "I'll be following along with: https://huggingface.co/learn/nlp-course/chapter4/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d546843-8220-4caa-b056-1fd53babb1d5",
   "metadata": {},
   "source": [
    "Browse https://huggingface.co/models for models, filter on task/trainingset/backbone. Use the widget to test the model from the webpage."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "461be445-2862-4e02-b288-265cae22b1ab",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Huggingface - NLP Crourse - Chapter 5: The 🤗 Datasets Library\n",
    "I'll be following along with: https://huggingface.co/learn/nlp-course/chapter5/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bb1b5d5-c9ce-4bb1-833f-07853a2e84a0",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Load a dataset from outside the 🤗 ecosystem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "8ff8f9da-ea2a-487f-846a-3d20112a72aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !wget https://github.com/crux82/squad-it/raw/master/SQuAD_it-train.json.gz\n",
    "# !wget https://github.com/crux82/squad-it/raw/master/SQuAD_it-test.json.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9957426-6551-4558-af4a-eac390668b04",
   "metadata": {},
   "source": [
    "### Read from File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "508bdc58-358d-4688-84e4-f07dd0c13eae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "data_files = {\"train\": \"SQuAD_it-train.json\", \"test\": \"SQuAD_it-test.json\"}\n",
    "squad_it_dataset = load_dataset(\"json\", data_files=data_files, field=\"data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6e4b6ec-aef5-4a09-adf8-2fd71706e5d5",
   "metadata": {},
   "source": [
    "### Read from Compressed File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "b25b5091-7bce-4793-a386-a37e9a1e11f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "data_files = {\"train\": \"SQuAD_it-train.json.gz\", \"test\": \"SQuAD_it-test.json.gz\"}\n",
    "squad_it_dataset = load_dataset(\"json\", data_files=data_files, field=\"data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9d513d3-f8af-4a02-b6f3-a8805d9ebf86",
   "metadata": {},
   "source": [
    "### Read from URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "a1496094-06ea-4d6a-830c-a4be29fad375",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "url = \"https://github.com/crux82/squad-it/raw/master/\"\n",
    "data_files = {\n",
    "    \"train\": url + \"SQuAD_it-train.json.gz\",\n",
    "    \"test\": url + \"SQuAD_it-test.json.gz\",\n",
    "}\n",
    "squad_it_dataset = load_dataset(\"json\", data_files=data_files, field=\"data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3bcb9d9-095b-4303-a0c2-459954d84a72",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Slice and Dice the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e38f74d-988f-43ac-800a-14fa25b039c7",
   "metadata": {},
   "source": [
    "### Shuffle the data\n",
    "This prevent the model from learning artificial ordering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "684538d5-4d9b-46ef-9572-484448e6f6c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['5733be284776f41900661182', '5733be284776f4190066117f', '5733be284776f41900661180', '5733be284776f41900661181', '5733be284776f4190066117e']\n",
      "['5727cc873acd2414000deca9', '5730b096396df919000962a0', '5729125daf94a219006aa029', '5727e9e0ff5b5019007d9852', '5731cca5e17f3d140042240a']\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "squad = load_dataset('squad', split='train')\n",
    "squad_shuffled = squad.shuffle(seed=666)\n",
    "\n",
    "print([s['id'] for s in squad.select(range(5))])\n",
    "print([s['id'] for s in squad_shuffled.select(range(5))])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec8249b9-eeef-4647-b8da-85f574df2c86",
   "metadata": {},
   "source": [
    "### Create a random train/test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "30c63a1b-da32-47dc-9047-3ad2b4e95105",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'title', 'context', 'question', 'answers'],\n",
       "        num_rows: 78839\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'title', 'context', 'question', 'answers'],\n",
       "        num_rows: 8760\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "squad = load_dataset('squad', split='train')\n",
    "dataset = squad.train_test_split(test_size=0.1)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3998242-4e0a-432c-84f2-04e8ccc46e56",
   "metadata": {},
   "source": [
    "### Select specific rows from the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "d7f7e5ae-4f64-4814-ae4d-2fe604ad5b81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['List_of_numbered_streets_in_Manhattan', 'LaserDisc', 'List_of_numbered_streets_in_Manhattan', 'Light-emitting_diode', 'LaserDisc']\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "squad = load_dataset('squad', split='train')\n",
    "squad_filtered = squad.filter(lambda x: x['title'].startswith('L'))\n",
    "print([s['title'] for s in squad_filtered.shuffle().select(range(5))])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e792673b-3764-4117-8d34-1aea0de97ca4",
   "metadata": {},
   "source": [
    "### Flatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "5fd49485-1fb6-40e7-b1c1-615870a8e50f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['id', 'title', 'context', 'question', 'answers'])\n",
      "dict_keys(['id', 'title', 'context', 'question', 'answers.text', 'answers.answer_start'])\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "squad = load_dataset('squad', split='train')\n",
    "print(squad.features.keys())\n",
    "squad_flattened = squad.flatten()\n",
    "print(squad_flattened.features.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9adb2ec-05f0-4548-be9f-ff5db1444e1b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Convert Dataset to Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "317663d4-7f5a-4584-803f-198d39e48dd1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>year</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>language</th>\n",
       "      <th>region</th>\n",
       "      <th>canton</th>\n",
       "      <th>legal area</th>\n",
       "      <th>source_language</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>2000</td>\n",
       "      <td>A.- Der 1955 geborene V._ war seit 1. Septembe...</td>\n",
       "      <td>0</td>\n",
       "      <td>de</td>\n",
       "      <td>Zürich</td>\n",
       "      <td>zh</td>\n",
       "      <td>insurance law</td>\n",
       "      <td>n/a</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  year                                               text  label  \\\n",
       "0   2  2000  A.- Der 1955 geborene V._ war seit 1. Septembe...      0   \n",
       "\n",
       "  language  region canton     legal area source_language  \n",
       "0       de  Zürich     zh  insurance law             n/a  "
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset('swiss_judgment_prediction', 'all', split='train')\n",
    "dataset.set_format('pandas')\n",
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "0cfcbfd7-4428-490c-9906-d1675965f1ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>year</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>language</th>\n",
       "      <th>region</th>\n",
       "      <th>canton</th>\n",
       "      <th>legal area</th>\n",
       "      <th>source_language</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>2000</td>\n",
       "      <td>A.- Der 1955 geborene V._ war seit 1. Septembe...</td>\n",
       "      <td>0</td>\n",
       "      <td>de</td>\n",
       "      <td>Zürich</td>\n",
       "      <td>zh</td>\n",
       "      <td>insurance law</td>\n",
       "      <td>n/a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>2000</td>\n",
       "      <td>Ansprüche nach OHG, hat sich ergeben: A.- X._ ...</td>\n",
       "      <td>1</td>\n",
       "      <td>de</td>\n",
       "      <td>Central Switzerland</td>\n",
       "      <td>lu</td>\n",
       "      <td>public law</td>\n",
       "      <td>n/a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>2000</td>\n",
       "      <td>Art. 4 aBV (Strafverfahren wegen falschen Zeug...</td>\n",
       "      <td>0</td>\n",
       "      <td>de</td>\n",
       "      <td>Northwestern Switzerland</td>\n",
       "      <td>ag</td>\n",
       "      <td>public law</td>\n",
       "      <td>n/a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>2000</td>\n",
       "      <td>Art. 5 Ziff. 1 EMRK (Haftentlassung), hat sich...</td>\n",
       "      <td>1</td>\n",
       "      <td>de</td>\n",
       "      <td>n/a</td>\n",
       "      <td>n/a</td>\n",
       "      <td>public law</td>\n",
       "      <td>n/a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6</td>\n",
       "      <td>2000</td>\n",
       "      <td>Mietvertrag, hat sich ergeben: A.- Die CT Cond...</td>\n",
       "      <td>0</td>\n",
       "      <td>de</td>\n",
       "      <td>n/a</td>\n",
       "      <td>n/a</td>\n",
       "      <td>civil law</td>\n",
       "      <td>n/a</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  year                                               text  label  \\\n",
       "0   2  2000  A.- Der 1955 geborene V._ war seit 1. Septembe...      0   \n",
       "1   3  2000  Ansprüche nach OHG, hat sich ergeben: A.- X._ ...      1   \n",
       "2   4  2000  Art. 4 aBV (Strafverfahren wegen falschen Zeug...      0   \n",
       "3   5  2000  Art. 5 Ziff. 1 EMRK (Haftentlassung), hat sich...      1   \n",
       "4   6  2000  Mietvertrag, hat sich ergeben: A.- Die CT Cond...      0   \n",
       "\n",
       "  language                    region canton     legal area source_language  \n",
       "0       de                    Zürich     zh  insurance law             n/a  \n",
       "1       de       Central Switzerland     lu     public law             n/a  \n",
       "2       de  Northwestern Switzerland     ag     public law             n/a  \n",
       "3       de                       n/a    n/a     public law             n/a  \n",
       "4       de                       n/a    n/a      civil law             n/a  "
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = dataset[:]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "a0bf68ff-52d4-41ad-a1d7-96315c575b84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "region                    language\n",
       "Central Switzerland       de           4778\n",
       "                          it              1\n",
       "Eastern Switzerland       de           5650\n",
       "                          it             57\n",
       "Espace Mittelland         de           5150\n",
       "                          fr           3104\n",
       "                          it              3\n",
       "Federation                de           1011\n",
       "                          fr            227\n",
       "                          it             70\n",
       "Northwestern Switzerland  de           5654\n",
       "                          fr              1\n",
       "Région lémanique          fr          13100\n",
       "                          de            336\n",
       "Ticino                    it           2249\n",
       "                          de              6\n",
       "Zürich                    de           8785\n",
       "                          fr              3\n",
       "n/a                       fr           4744\n",
       "                          de           4088\n",
       "                          it            692\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby('region')['language'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "de500ffb-d8db-43c5-b077-82f7ee6bed18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "legal area\n",
       "public law       15173\n",
       "penal law        11795\n",
       "civil law        11477\n",
       "insurance law    11142\n",
       "social law        9727\n",
       "other              395\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['legal area'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "60ef4f70-10d2-4806-9176-b08f8fbf9c66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# when done, get the format back to default\n",
    "dataset.reset_format()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f42637c-17bf-49ec-b2fd-94295a818c27",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Saving/Reload a dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "3988b9b8-e6c8-4213-bd21-f78331910a1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset('allocine')\n",
    "# print(ds.cache_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aff427da-27bd-4a4b-b39e-5b278625eb77",
   "metadata": {},
   "source": [
    "#### Apache Arrow\n",
    "Default efficient format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "1ad7a420-6205-455a-90a0-1e8b2766953a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14ba50312b2e408a92d1374ec6597696",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/160000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b41d8bc130c4255958f6a85bc99e2ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/20000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ceefd45306ee4d4f9479d308a32dfad0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/20000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ds.save_to_disk('local-allocine')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "a96c13a0-d76f-42cf-a55b-b6cc944a7457",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_from_disk\n",
    "\n",
    "arrow_ds = load_from_disk('local-allocine')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58118760-bea9-4e6d-9267-ef27abce44d4",
   "metadata": {},
   "source": [
    "#### CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "e474c28b-4e40-4a58-a16c-e80553b2f3e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e51e88db61c94468955c7f81be60efed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating CSV from Arrow format:   0%|          | 0/160 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce36cd03b6484d93a55f872aa57b0e2f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating CSV from Arrow format:   0%|          | 0/20 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63155a0d3909424d986c0542436b7e65",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating CSV from Arrow format:   0%|          | 0/20 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for split, dataset in ds.items():\n",
    "  dataset.to_csv(f'local-allocine-{split}.csv', index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "3ad9aff2-425f-4a6d-ba13-5c3bd2081aad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d190c702e784491b840886d43e8a5f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6da321229b1a48a7bb94f89a506f0b4e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06af1808f9634d33978459eccc5de8a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28b95be32032423ca929e601a6f3058e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99df09f6684848aaae1da897f4fb3b30",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data_files = {\n",
    "  'train': 'local-allocine-train.csv',\n",
    "  'validation': 'local-allocine-validation.csv',\n",
    "  'test': 'local-allocine-test.csv',\n",
    "}\n",
    "\n",
    "csv_ds = load_dataset('csv', data_files=data_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2227c34-91ee-4a57-8243-f7905c01f302",
   "metadata": {},
   "source": [
    "#### JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "4f526e81-0a9c-4745-acc1-a813545ff8cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "473288ff2bb54df6a18579d75a5b4148",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating json from Arrow format:   0%|          | 0/160 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2ef605edf2b449388f0819517ed5849",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating json from Arrow format:   0%|          | 0/20 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6fafbb74b54485aaa29276dca2bd214",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating json from Arrow format:   0%|          | 0/20 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for split, dataset in ds.items():\n",
    "  dataset.to_json(f'local-allocine-{split}.jsonl', index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "97b97d42-236b-4f22-8dcb-6caa705dd22a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1867c1d48a24494ae7183a6b9954241",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b36733da4ad40f79440779f675d108d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "483fc209f684481789b6cccd741724c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9bed3ed914c04e349d196f9f53dd78ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e72aa25555147a0a9f9ededfba6f8d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data_files = {\n",
    "  'train': 'local-allocine-train.jsonl',\n",
    "  'validation': 'local-allocine-validation.jsonl',\n",
    "  'test': 'local-allocine-test.jsonl',\n",
    "}\n",
    "\n",
    "json_ds = load_dataset('json', data_files=data_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85cca254-44e0-4221-b53f-e7d39069742f",
   "metadata": {},
   "source": [
    "#### Parquet\n",
    "For long term (size efficient) storage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "1d9822ff-1007-4071-93be-c1c069125ee9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a15458312aa46b28da11cf04d8da4f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/160 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0d237400ccf4e4aa476782d54c025c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/20 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e030f3c79ad34362ab4b1d85475332f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/20 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for split, dataset in ds.items():\n",
    "  dataset.to_parquet(f'local-allocine-{split}.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "f215e757-dd31-4d37-a570-7fd06cd4d3a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40b2277f81cd4b8a93372550b3971f7d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "654bf79a291f44988b2527f5531be7a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8462bf70ace4712adf95d01de36c8bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d525e85da7584fdaaa658ba569b9901f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5779549c1c1a43209008f2fa22a0ee90",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data_files = {\n",
    "  'train': 'local-allocine-train.parquet',\n",
    "  'validation': 'local-allocine-validation.parquet',\n",
    "  'test': 'local-allocine-test.parquet',\n",
    "}\n",
    "\n",
    "parquet_ds = load_dataset('parquet', data_files=data_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "538aab2f-d381-4385-b16c-8ff450dca400",
   "metadata": {},
   "source": [
    "## Semantic Search with FAISS\n",
    "Using an encoder (e.g. BERT) we can convert an input sentence into an \"embedding\" (aka. a vector of integers). We can compute the similarities between different embeddings by computing the angles between them (smaller angle means more similar).\n",
    "\n",
    "The example take the github issues with at least 15 words. Concat the title body and comments into one string. Compute embeddings of it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "ae16587a-d6f6-4603-be2c-c0204b4320f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install faiss-cpu faiss-gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "195a7d2b-ad04-46f5-92ab-b68d22aecc97",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Repo card metadata block was not found. Setting CardData to empty.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>html_url</th>\n",
       "      <th>title</th>\n",
       "      <th>comments</th>\n",
       "      <th>body</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://github.com/huggingface/datasets/issues...</td>\n",
       "      <td>Protect master branch</td>\n",
       "      <td>Cool, I think we can do both :)</td>\n",
       "      <td>After accidental merge commit (91c55355b634d0d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://github.com/huggingface/datasets/issues...</td>\n",
       "      <td>Protect master branch</td>\n",
       "      <td>@lhoestq now the 2 are implemented.\\r\\n\\r\\nPle...</td>\n",
       "      <td>After accidental merge commit (91c55355b634d0d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://github.com/huggingface/datasets/issues...</td>\n",
       "      <td>Backwards compatibility broken for cached data...</td>\n",
       "      <td>Hi ! I guess the caching mechanism should have...</td>\n",
       "      <td>## Describe the bug\\r\\nAfter upgrading to data...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://github.com/huggingface/datasets/issues...</td>\n",
       "      <td>Backwards compatibility broken for cached data...</td>\n",
       "      <td>If it's easy enough to implement, then yes ple...</td>\n",
       "      <td>## Describe the bug\\r\\nAfter upgrading to data...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            html_url  \\\n",
       "0  https://github.com/huggingface/datasets/issues...   \n",
       "1  https://github.com/huggingface/datasets/issues...   \n",
       "2  https://github.com/huggingface/datasets/issues...   \n",
       "3  https://github.com/huggingface/datasets/issues...   \n",
       "\n",
       "                                               title  \\\n",
       "0                              Protect master branch   \n",
       "1                              Protect master branch   \n",
       "2  Backwards compatibility broken for cached data...   \n",
       "3  Backwards compatibility broken for cached data...   \n",
       "\n",
       "                                            comments  \\\n",
       "0                    Cool, I think we can do both :)   \n",
       "1  @lhoestq now the 2 are implemented.\\r\\n\\r\\nPle...   \n",
       "2  Hi ! I guess the caching mechanism should have...   \n",
       "3  If it's easy enough to implement, then yes ple...   \n",
       "\n",
       "                                                body  \n",
       "0  After accidental merge commit (91c55355b634d0d...  \n",
       "1  After accidental merge commit (91c55355b634d0d...  \n",
       "2  ## Describe the bug\\r\\nAfter upgrading to data...  \n",
       "3  ## Describe the bug\\r\\nAfter upgrading to data...  "
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "issues_dataset = load_dataset('lewtun/github-issues', split='train')\n",
    "issues_dataset = issues_dataset.filter(lambda x: (x['is_pull_request'] == False and len(x['comments']) > 0))\n",
    "columns = issues_dataset.column_names\n",
    "columns_to_keep = ['title', 'body', 'html_url', 'comments']\n",
    "columns_to_remove = set(columns_to_keep).symmetric_difference(columns)\n",
    "issues_dataset = issues_dataset.remove_columns(columns_to_remove)\n",
    "issues_dataset.set_format('pandas')\n",
    "df = issues_dataset[:]\n",
    "comments_df = df.explode('comments', ignore_index=True)\n",
    "comments_df.head(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "6ea174b0-38fc-4acf-ba14-522890b3c926",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52a9f5908a804a8fa272c48d7899ab90",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2964 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c27d747c09445deb3e7857e6362e198",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/2964 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3d100f5b5fa47448319e198dcb5a975",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2175 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "comments_dataset = Dataset.from_pandas(comments_df)\n",
    "# keep comments with at least 15 words\n",
    "comments_dataset = comments_dataset.map(lambda x: {'comment_length': len(x['comments'].split())})\n",
    "comments_dataset = comments_dataset.filter(lambda x: x[\"comment_length\"] > 15)\n",
    "\n",
    "def concatenate_text(examples):\n",
    "    return {\n",
    "        \"text\": examples[\"title\"]\n",
    "        + \" \\n \"\n",
    "        + examples[\"body\"]\n",
    "        + \" \\n \"\n",
    "        + examples[\"comments\"]\n",
    "    }\n",
    "comments_dataset = comments_dataset.map(concatenate_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "92aeef2d-7a82-4af9-9244-9f7a357f4481",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 768])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "\n",
    "model_ckpt = 'sentence-transformers/multi-qa-mpnet-base-dot-v1'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_ckpt)\n",
    "model = AutoModel.from_pretrained(model_ckpt)\n",
    "\n",
    "# device = torch.device('cuda')\n",
    "# windows doesn't have faiss-gpu ...\n",
    "device = torch.device('cpu')\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "def cls_pooling(model_output):\n",
    "    return model_output.last_hidden_state[:, 0]\n",
    "\n",
    "def get_embeddings(text_list):\n",
    "    encoded_input = tokenizer(text_list, padding=True, truncation=True, return_tensors='pt')\n",
    "    encoded_input = {k: v.to(device) for k, v in encoded_input.items()}\n",
    "    model_output = model(**encoded_input)\n",
    "    return cls_pooling(model_output)\n",
    "\n",
    "embedding = get_embeddings(comments_dataset['text'][0])\n",
    "embedding.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "71e8684a-ab1a-40bf-b24c-aadc8207a3e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f90bd4720bf406bbbb5587cf7e40e97",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# truncate the dataset because CPU takes 35min to compute the embeddings otherwise\n",
    "comments_dataset = comments_dataset.select(range(100))\n",
    "embeddings_dataset = comments_dataset.map(\n",
    "    lambda x: {'embeddings': get_embeddings(x[\"text\"]).detach().cpu().numpy()[0]}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "68089d4c-2492-4384-9c6e-4269c42e9bb4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4edded556aa04ca9b5ace04ab0e4190a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['html_url', 'title', 'comments', 'body', 'comment_length', 'text', 'embeddings'],\n",
       "    num_rows: 100\n",
       "})"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings_dataset.add_faiss_index(column='embeddings')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "3305761e-3a5a-4b38-9b45-2fb4de090576",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 768)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = 'How can I load a dataset offline?'\n",
    "question_embedding = get_embeddings([question]).cpu().detach().numpy()\n",
    "question_embedding.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "ffc30145-4c3f-40a1-bb1b-80f87c4fb9fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "scores, samples = embeddings_dataset.get_nearest_examples('embeddings', question_embedding, k=5)\n",
    "samples_df = pd.DataFrame.from_dict(samples)\n",
    "samples_df[\"scores\"] = scores\n",
    "samples_df.sort_values(\"scores\", ascending=False, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "a4d6fb74-b270-40c2-b96a-f47535f9db78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COMMENT: Thanks for reporting ! #2852 fixed this error\n",
      "\n",
      "We'll do a new release of `datasets` soon :)\n",
      "SCORE: 43.938785552978516\n",
      "TITLE: Cannot load linnaeus dataset\n",
      "URL: https://github.com/huggingface/datasets/issues/2821\n",
      "==================================================\n",
      "\n",
      "COMMENT: > * For the platform, we need to know the operating system of your machine. Could you please run the command `datasets-cli env` and copy-and-paste its output below?\n",
      "> * In relation with the error, you just gave us the error type and message (`TypeError: 'NoneType' object is not callable`). Could you please copy-paste the complete stack trace, so that we know exactly which part of the code threw the error?\n",
      "\n",
      "1. For the platform, here are the output:\n",
      "        - datasets` version: 1.11.0\n",
      "        - Platform: Windows-10-10.0.19041-SP0\n",
      "        - Python version: 3.7.10\n",
      "        - PyArrow version: 5.0.0\n",
      "2. For the code and error：\n",
      "     ```python\n",
      "     from datasets import load_dataset, load_metric\n",
      "     dataset = load_dataset(\"glue\", \"cola\")\n",
      "    ```\n",
      "    ```python\n",
      "    Traceback (most recent call last):\n",
      "    ....\n",
      "    ....\n",
      "    File \"my_file.py\", line 2, in <module>\n",
      "    dataset = load_dataset(\"glue\", \"cola\")\n",
      "    File \"My environments\\lib\\site-packages\\datasets\\load.py\", line 830, in load_dataset\n",
      "    **config_kwargs,\n",
      "     File \"My environments\\lib\\site-packages\\datasets\\load.py\", line 710, in load_dataset_builder\n",
      "    **config_kwargs,\n",
      "    TypeError: 'NoneType' object is not callable\n",
      "    ```\n",
      "   Thank you!\n",
      "SCORE: 43.24859619140625\n",
      "TITLE: TypeError: 'NoneType' object is not callable\n",
      "URL: https://github.com/huggingface/datasets/issues/2869\n",
      "==================================================\n",
      "\n",
      "COMMENT: > * For the platform, we need to know the operating system of your machine. Could you please run the command `datasets-cli env` and copy-and-paste its output below?\n",
      "> * In relation with the error, you just gave us the error type and message (`TypeError: 'NoneType' object is not callable`). Could you please copy-paste the complete stack trace, so that we know exactly which part of the code threw the error?\n",
      "\n",
      "1. For the platform, here are the output:\n",
      "        - datasets` version: 1.11.0\n",
      "        - Platform: Windows-10-10.0.19041-SP0\n",
      "        - Python version: 3.7.10\n",
      "        - PyArrow version: 5.0.0\n",
      "2. For the code and error：\n",
      "     ```python\n",
      "     from datasets import load_dataset, load_metric\n",
      "     dataset = load_dataset(\"glue\", \"cola\")\n",
      "    ```\n",
      "    ```python\n",
      "    Traceback (most recent call last):\n",
      "    ....\n",
      "    ....\n",
      "    File \"my_file.py\", line 2, in <module>\n",
      "    dataset = load_dataset(\"glue\", \"cola\")\n",
      "    File \"My environments\\lib\\site-packages\\datasets\\load.py\", line 830, in load_dataset\n",
      "    **config_kwargs,\n",
      "     File \"My environments\\lib\\site-packages\\datasets\\load.py\", line 710, in load_dataset_builder\n",
      "    **config_kwargs,\n",
      "    TypeError: 'NoneType' object is not callable\n",
      "    ```\n",
      "   Thank you!\n",
      "SCORE: 43.24859619140625\n",
      "TITLE: TypeError: 'NoneType' object is not callable\n",
      "URL: https://github.com/huggingface/datasets/issues/2869\n",
      "==================================================\n",
      "\n",
      "COMMENT: One naive question: do you have internet access from the machine where you execute the code?\n",
      "SCORE: 42.89968490600586\n",
      "TITLE: TypeError: 'NoneType' object is not callable\n",
      "URL: https://github.com/huggingface/datasets/issues/2869\n",
      "==================================================\n",
      "\n",
      "COMMENT: One naive question: do you have internet access from the machine where you execute the code?\n",
      "SCORE: 42.89968490600586\n",
      "TITLE: TypeError: 'NoneType' object is not callable\n",
      "URL: https://github.com/huggingface/datasets/issues/2869\n",
      "==================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for _, row in samples_df.iterrows():\n",
    "    print(f\"COMMENT: {row.comments}\")\n",
    "    print(f\"SCORE: {row.scores}\")\n",
    "    print(f\"TITLE: {row.title}\")\n",
    "    print(f\"URL: {row.html_url}\")\n",
    "    print(\"=\" * 50)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bd9e28c-6293-435f-82f7-2c192bf96804",
   "metadata": {},
   "source": [
    "# Huggingface - NLP Crourse - Chapter 6: The 🤗 Tokenizers Library\n",
    "I'll be following along with: https://huggingface.co/learn/nlp-course/chapter6/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e585948-ed60-4bb3-a6fd-8cb46840ddfe",
   "metadata": {},
   "source": [
    "Training a new Tokenizer is necessary if the corpus you want to train on is in a different language / charset / domain (e.g. medical) / style (e.g. old english)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "519b0db7-d856-4a5b-89ec-652bd60113a6",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Train a Tokenizer for Python code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4f15700f-5ce1-453e-856b-df6a25749ddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "raw_datasets = load_dataset('code_search_net', 'python')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9310721a-caae-458b-b70d-b8a94da6c4d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('code-search-net-tokenizer\\\\tokenizer_config.json',\n",
       " 'code-search-net-tokenizer\\\\special_tokens_map.json',\n",
       " 'code-search-net-tokenizer\\\\vocab.json',\n",
       " 'code-search-net-tokenizer\\\\merges.txt',\n",
       " 'code-search-net-tokenizer\\\\added_tokens.json',\n",
       " 'code-search-net-tokenizer\\\\tokenizer.json')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "def get_training_corpus():\n",
    "  dataset = raw_datasets['train']\n",
    "  for start_idx in range(0, len(dataset), 1000):\n",
    "    samples = dataset[start_idx: start_idx + 1000]\n",
    "    yield samples['whole_func_string']\n",
    "\n",
    "training_corpus = get_training_corpus()\n",
    "old_tokenizer = AutoTokenizer.from_pretrained('gpt2')\n",
    "new_tokenizer = old_tokenizer.train_new_from_iterator(training_corpus, 52000) # 52000 is the vocabulary's size\n",
    "new_tokenizer.save_pretrained('code-search-net-tokenizer')\n",
    "# new_tokenizer.push_to_hub('code-search-net-tokenizer')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0b8a1c5-572c-47bd-a110-646eee72603c",
   "metadata": {},
   "source": [
    "### Before and after comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7b226518-05c7-4ee5-a67a-d7a62ae48ebd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['def', 'Ġadd', '_', 'n', 'umbers', '(', 'a', ',', 'Ġb', '):', 'Ċ', 'Ġ', 'Ġ', 'Ġ', 'Ġ\"\"\"', 'Add', 'Ġthe', 'Ġtwo', 'Ġnumbers', 'Ġ`', 'a', '`', 'Ġand', 'Ġ`', 'b', '`', '.\"', '\"\"', 'Ċ', 'Ġ', 'Ġ', 'Ġ', 'Ġreturn', 'Ġa', 'Ġ+', 'Ġb']\n"
     ]
    }
   ],
   "source": [
    "example = '''def add_numbers(a, b):\n",
    "    \"\"\"Add the two numbers `a` and `b`.\"\"\"\n",
    "    return a + b'''\n",
    "\n",
    "tokens = old_tokenizer.tokenize(example)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "64759f0b-6ed8-4990-9d94-4fb7eeabdf64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['def', 'Ġadd', '_', 'numbers', '(', 'a', ',', 'Ġb', '):', 'ĊĠĠĠ', 'Ġ\"\"\"', 'Add', 'Ġthe', 'Ġtwo', 'Ġnumbers', 'Ġ`', 'a', '`', 'Ġand', 'Ġ`', 'b', '`.\"\"\"', 'ĊĠĠĠ', 'Ġreturn', 'Ġa', 'Ġ+', 'Ġb']\n"
     ]
    }
   ],
   "source": [
    "tokens = new_tokenizer.tokenize(example)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbaa26ed-a9f0-4451-8ef5-02a6e5d74c50",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Recreating `aggregation_strategy='simple'`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b8b14273-4d06-4992-b7ba-d1d9c902a462",
   "metadata": {},
   "outputs": [],
   "source": [
    "example = 'My name is Sylvain and I work at Hugging Face in Brooklyn.'\n",
    "checkpoint = \"dbmdz/bert-large-cased-finetuned-conll03-english\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68ad1f33-5c40-48c6-900f-037a41b78910",
   "metadata": {},
   "source": [
    "### Goal\n",
    "given naive token, group the semanticaly relevent tokens together.\n",
    "\n",
    "Instead of `['S', '##yl', '##va', '##in']` -> `['Sylvain']`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "18f86161-06a4-444d-885e-6fd779645924",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at dbmdz/bert-large-cased-finetuned-conll03-english were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "example='My name is Sylvain and I work at Hugging Face in Brooklyn.'\n",
      "[{'entity': 'I-PER', 'score': 0.99938285, 'index': 4, 'word': 'S', 'start': 11, 'end': 12}\n",
      "{'entity': 'I-PER', 'score': 0.99815494, 'index': 5, 'word': '##yl', 'start': 12, 'end': 14}\n",
      "{'entity': 'I-PER', 'score': 0.99590707, 'index': 6, 'word': '##va', 'start': 14, 'end': 16}\n",
      "{'entity': 'I-PER', 'score': 0.99923277, 'index': 7, 'word': '##in', 'start': 16, 'end': 18}\n",
      "{'entity': 'I-ORG', 'score': 0.9738931, 'index': 12, 'word': 'Hu', 'start': 33, 'end': 35}\n",
      "{'entity': 'I-ORG', 'score': 0.976115, 'index': 13, 'word': '##gging', 'start': 35, 'end': 40}\n",
      "{'entity': 'I-ORG', 'score': 0.9887976, 'index': 14, 'word': 'Face', 'start': 41, 'end': 45}\n",
      "{'entity': 'I-LOC', 'score': 0.9932106, 'index': 16, 'word': 'Brooklyn', 'start': 49, 'end': 57}]\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "token_classifier = pipeline('token-classification', model=checkpoint, tokenizer=checkpoint)\n",
    "tokens = token_classifier(example)\n",
    "print(f'{example=}\\n[' + '\\n'.join(str(t) for t in tokens) + ']')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "94d85647-543e-4982-82fb-8400a01fe694",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at dbmdz/bert-large-cased-finetuned-conll03-english were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "example='My name is Sylvain and I work at Hugging Face in Brooklyn.'\n",
      "[{'entity_group': 'PER', 'score': 0.9981694, 'word': 'Sylvain', 'start': 11, 'end': 18}\n",
      "{'entity_group': 'ORG', 'score': 0.9796019, 'word': 'Hugging Face', 'start': 33, 'end': 45}\n",
      "{'entity_group': 'LOC', 'score': 0.9932106, 'word': 'Brooklyn', 'start': 49, 'end': 57}]\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "token_classifier = pipeline('token-classification', model=checkpoint, tokenizer=checkpoint, aggregation_strategy='simple')\n",
    "tokens = token_classifier(example)\n",
    "print(f'{example=}\\n[' + '\\n'.join(str(t) for t in tokens) + ']')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4efe5835-80fd-4ca8-ba61-720fe4970836",
   "metadata": {},
   "source": [
    "### Manual-ish re-implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5f04d767-ba94-4e8c-82cf-48873723db34",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at dbmdz/bert-large-cased-finetuned-conll03-english were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "example='My name is Sylvain and I work at Hugging Face in Brooklyn.'\n",
      "[{'entity_group': 'PER', 'score': 0.9981694221496582, 'word': 'Sylvain', 'start': 11, 'end': 18}\n",
      "{'entity_group': 'ORG', 'score': 0.9796019196510315, 'word': 'Hugging Face', 'start': 33, 'end': 45}\n",
      "{'entity_group': 'LOC', 'score': 0.9932106137275696, 'word': 'Brooklyn', 'start': 49, 'end': 57}]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "import numpy as np\n",
    "\n",
    "results = []\n",
    "token_classifier = pipeline('token-classification', model=checkpoint, tokenizer=checkpoint)\n",
    "tokens = token_classifier(example)\n",
    "\n",
    "idx = 0\n",
    "while idx < len(tokens):\n",
    "    label = tokens[idx]['entity']\n",
    "    # Remove the B- or I-\n",
    "    label = label[2:]\n",
    "    start = tokens[idx]['start']\n",
    "\n",
    "    # Grab all the tokens labeled with I-label\n",
    "    all_scores = []\n",
    "    while idx < len(tokens) and tokens[idx]['entity'] == f'I-{label}':\n",
    "      all_scores.append(tokens[idx]['score'])\n",
    "      end = tokens[idx]['end']\n",
    "      idx += 1\n",
    "\n",
    "    # The score is the mean of all the scores of the tokens in that grouped entity\n",
    "    score = np.mean(all_scores).item()\n",
    "    word = example[start: end]\n",
    "    results.append(\n",
    "      {\n",
    "        \"entity_group\": label,\n",
    "        \"score\": score,\n",
    "        \"word\": word,\n",
    "        \"start\": start,\n",
    "        \"end\": end,\n",
    "    })\n",
    "\n",
    "print(f'{example=}\\n[' + '\\n'.join(str(t) for t in results) + ']')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1780984-cd2c-4689-adaa-12f97e91be7c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Find the Answer to a Question in a Context"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "768fc594-d198-466b-b2b6-d8ddf08ed46b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Goal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e3d69975-e0c8-49cd-b2e8-e39a739fbe25",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert-base-cased-distilled-squad and revision 626af31 (https://huggingface.co/distilbert-base-cased-distilled-squad).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'score': 0.9802601933479309,\n",
       " 'start': 78,\n",
       " 'end': 106,\n",
       " 'answer': 'Jax, PyTorch, and TensorFlow'}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "question_answerer = pipeline(\"question-answering\")\n",
    "context = \"\"\"\n",
    "🤗 Transformers is backed by the three most popular deep learning libraries — Jax, PyTorch, and TensorFlow — with a seamless integration\n",
    "between them. It's straightforward to train your models with one before loading them for inference with the other.\n",
    "\"\"\"\n",
    "question = \"Which deep learning libraries back 🤗 Transformers?\"\n",
    "question_answerer(question=question, context=context)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f2d09df-95a4-481f-b3aa-e746fd1d86fa",
   "metadata": {},
   "source": [
    "### Manual-ish re-implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "18ab1388-bdca-4264-9667-ab59bb8510ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 67]) torch.Size([1, 67])\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForQuestionAnswering\n",
    "\n",
    "model_checkpoint = \"distilbert-base-cased-distilled-squad\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(model_checkpoint)\n",
    "\n",
    "inputs = tokenizer(question, context, return_tensors=\"pt\")\n",
    "outputs = model(**inputs)\n",
    "\n",
    "start_logits = outputs.start_logits\n",
    "end_logits = outputs.end_logits\n",
    "# First logits represent the start token idx of the answer, second the end token idx.\n",
    "print(start_logits.shape, end_logits.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "8d9419a8-6d45-4c18-8a66-a87901c5316b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([4.4531e-07, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 8.1185e-06, 1.3470e-05,\n",
      "        2.4369e-07, 2.1236e-06, 1.3220e-06, 3.7722e-04, 6.9220e-03, 1.0237e-05,\n",
      "        4.3289e-06, 1.5143e-05, 3.2464e-07, 4.1933e-06, 1.6808e-04, 9.9179e-01,\n",
      "        8.6288e-06, 3.8557e-04, 5.9956e-06, 4.3725e-06, 5.8977e-07, 3.0929e-06,\n",
      "        3.8999e-06, 2.9493e-06, 2.1940e-04, 5.4713e-06, 7.1354e-06, 2.3212e-05,\n",
      "        5.2711e-06, 4.7788e-07, 2.4292e-07, 4.4467e-07, 1.4879e-08, 4.8133e-08,\n",
      "        3.7169e-07, 7.1242e-08, 3.1735e-07, 2.2365e-07, 1.3685e-06, 2.4093e-08,\n",
      "        1.1470e-08, 4.4891e-07, 2.2828e-08, 5.2562e-07, 5.8093e-07, 1.6419e-06,\n",
      "        1.4114e-08, 2.0591e-07, 2.0161e-08, 2.5390e-07, 2.3251e-08, 1.4667e-08,\n",
      "        5.4533e-08, 2.4235e-08, 5.5390e-09, 1.8524e-08, 3.6818e-08, 3.4721e-08,\n",
      "        0.0000e+00], grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "sequence_ids = inputs.sequence_ids()\n",
    "# Mask everything apart from the tokens of the context\n",
    "mask = [i != 1 for i in sequence_ids]\n",
    "# Unmask the [CLS] token\n",
    "mask[0] = False\n",
    "mask = torch.tensor(mask)[None]\n",
    "\n",
    "start_logits[mask] = -10000\n",
    "end_logits[mask] = -10000\n",
    "\n",
    "start_probabilities = torch.nn.functional.softmax(start_logits, dim=-1)[0]\n",
    "end_probabilities = torch.nn.functional.softmax(end_logits, dim=-1)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a149d1ba-284b-475f-842e-175ce753eb52",
   "metadata": {},
   "source": [
    "#### Enforce some invarient\n",
    "We want to make sure that start_idx < end_idx. So we compute the probability of all pairs of `(start, end)` NOP-ing the ones where `start_idx >= end_idx` and pick a winner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "b4f64d2c-b517-4e51-ae71-965286f9aed7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start_index=23 end_index=35 scores[start_index, end_index]=tensor(0.9803, grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Compute all combinations\n",
    "scores = start_probabilities[:, None] * end_probabilities[None, :]\n",
    "# NOP the lower triangular part\n",
    "scores = torch.triu(scores)\n",
    "\n",
    "# Get the index of the winner\n",
    "max_index = scores.argmax().item()\n",
    "start_index = max_index // scores.shape[1]\n",
    "end_index = max_index % scores.shape[1]\n",
    "print(f'{start_index=} {end_index=} {scores[start_index, end_index]=}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "a5747570-b550-4795-8269-a1cbc6e6aaea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'answer': 'Jax, PyTorch, and TensorFlow', 'start': 78, 'end': 106, 'score': tensor(0.9803, grad_fn=<SelectBackward0>)}\n"
     ]
    }
   ],
   "source": [
    "inputs_with_offsets = tokenizer(question, context, return_offsets_mapping=True)\n",
    "offsets = inputs_with_offsets[\"offset_mapping\"]\n",
    "\n",
    "start_char, _ = offsets[start_index]\n",
    "_, end_char = offsets[end_index]\n",
    "answer = context[start_char:end_char]\n",
    "\n",
    "result = {\n",
    "    \"answer\": answer,\n",
    "    \"start\": start_char,\n",
    "    \"end\": end_char,\n",
    "    \"score\": scores[start_index, end_index],\n",
    "}\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86a41c8e-486a-4903-9190-fb444e3ad942",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Handling long context\n",
    "If the context + question has more tokens than the model can take we can chunk the context and do multiple queries. To prevent the answer to be cut in between chunks we generate overlapping chunks of context."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ee959a8-7a80-4d14-a504-9cc10c3707ac",
   "metadata": {},
   "source": [
    "### Goal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "61b25ee0-828e-4d1f-a6fe-6843f41cade6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'score': 0.9714871048927307,\n",
       " 'start': 1892,\n",
       " 'end': 1919,\n",
       " 'answer': 'Jax, PyTorch and TensorFlow'}"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "long_context = \"\"\"\n",
    "🤗 Transformers: State of the Art NLP\n",
    "\n",
    "🤗 Transformers provides thousands of pretrained models to perform tasks on texts such as classification, information extraction,\n",
    "question answering, summarization, translation, text generation and more in over 100 languages.\n",
    "Its aim is to make cutting-edge NLP easier to use for everyone.\n",
    "\n",
    "🤗 Transformers provides APIs to quickly download and use those pretrained models on a given text, fine-tune them on your own datasets and\n",
    "then share them with the community on our model hub. At the same time, each python module defining an architecture is fully standalone and\n",
    "can be modified to enable quick research experiments.\n",
    "\n",
    "Why should I use transformers?\n",
    "\n",
    "1. Easy-to-use state-of-the-art models:\n",
    "  - High performance on NLU and NLG tasks.\n",
    "  - Low barrier to entry for educators and practitioners.\n",
    "  - Few user-facing abstractions with just three classes to learn.\n",
    "  - A unified API for using all our pretrained models.\n",
    "  - Lower compute costs, smaller carbon footprint:\n",
    "\n",
    "2. Researchers can share trained models instead of always retraining.\n",
    "  - Practitioners can reduce compute time and production costs.\n",
    "  - Dozens of architectures with over 10,000 pretrained models, some in more than 100 languages.\n",
    "\n",
    "3. Choose the right framework for every part of a model's lifetime:\n",
    "  - Train state-of-the-art models in 3 lines of code.\n",
    "  - Move a single model between TF2.0/PyTorch frameworks at will.\n",
    "  - Seamlessly pick the right framework for training, evaluation and production.\n",
    "\n",
    "4. Easily customize a model or an example to your needs:\n",
    "  - We provide examples for each architecture to reproduce the results published by its original authors.\n",
    "  - Model internals are exposed as consistently as possible.\n",
    "  - Model files can be used independently of the library for quick experiments.\n",
    "\n",
    "🤗 Transformers is backed by the three most popular deep learning libraries — Jax, PyTorch and TensorFlow — with a seamless integration\n",
    "between them. It's straightforward to train your models with one before loading them for inference with the other.\n",
    "\"\"\"\n",
    "question_answerer(question=question, context=long_context)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31c7d669-aca9-4001-8e30-01561dbb90cb",
   "metadata": {},
   "source": [
    "### Manual-ish re-implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a494cba4-21ce-4348-902a-1a226bdef3f8",
   "metadata": {},
   "source": [
    "#### Small examples for Intuition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "5388d4ec-ff41-4055-9b69-06cf87db2466",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] This sentence is not [SEP]\n",
      "[CLS] is not too long [SEP]\n",
      "[CLS] too long but we [SEP]\n",
      "[CLS] but we are going [SEP]\n",
      "[CLS] are going to split [SEP]\n",
      "[CLS] to split it anyway [SEP]\n",
      "[CLS] it anyway. [SEP]\n"
     ]
    }
   ],
   "source": [
    "sentence = \"This sentence is not too long but we are going to split it anyway.\"\n",
    "inputs = tokenizer(sentence, truncation=True, return_overflowing_tokens=True, max_length=6, stride=2)\n",
    "\n",
    "for ids in inputs[\"input_ids\"]:\n",
    "    print(tokenizer.decode(ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "2daa27d0-30b8-4e44-92d8-c114f38aa648",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starts=tensor([1, 2, 3, 4]) reshaped as a column tensor([[1],\n",
      "        [2],\n",
      "        [3],\n",
      "        [4]])\n",
      "cross product:\n",
      "tensor([[ 5,  6,  7,  8],\n",
      "        [10, 12, 14, 16],\n",
      "        [15, 18, 21, 24],\n",
      "        [20, 24, 28, 32]])\n",
      "upper triangle:\n",
      "tensor([[ 5,  6,  7,  8],\n",
      "        [ 0, 12, 14, 16],\n",
      "        [ 0,  0, 21, 24],\n",
      "        [ 0,  0,  0, 32]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "starts = torch.tensor([1, 2, 3, 4])\n",
    "ends = torch.tensor([5, 6, 7, 8])\n",
    "print(f'{starts=} reshaped as a column {starts[:, None]}')\n",
    "\n",
    "# Compute the cross product\n",
    "# [[1],                        [[1 * 5,   1 * 6,   1 * 7,   1 * 8],\n",
    "#  [2],  x  [[5, 6, 7, 8]]  =   [2 * 5,   2 * 6,   2 * 7,   2 * 8],\n",
    "#  [3],                         [3 * 5,   3 * 6,   3 * 7,   3 * 8],\n",
    "#  [4]]                         [4 * 5,   4 * 6,   4 * 7,   4 * 8]]\n",
    "cross = starts[:, None] * ends[None, :]\n",
    "print(f'cross product:\\n{cross}')\n",
    "\n",
    "# NOP the bottom triangle\n",
    "cross = torch.triu(cross)\n",
    "print(f'upper triangle:\\n{cross}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06b6fd90-f4b9-4a67-bf47-fcb7b27b9773",
   "metadata": {},
   "source": [
    "#### Actual Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "a885eb4f-7d14-44fe-b731-0ff2a10aa2e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 384])\n"
     ]
    }
   ],
   "source": [
    "inputs = tokenizer(\n",
    "    question,\n",
    "    long_context,\n",
    "    stride=128,\n",
    "    max_length=384,\n",
    "    padding=\"longest\",\n",
    "    truncation=\"only_second\",\n",
    "    return_overflowing_tokens=True,\n",
    "    return_offsets_mapping=True,\n",
    ")\n",
    "\n",
    "_ = inputs.pop(\"overflow_to_sample_mapping\")\n",
    "offsets = inputs.pop(\"offset_mapping\")\n",
    "\n",
    "inputs = inputs.convert_to_tensors(\"pt\")\n",
    "print(inputs[\"input_ids\"].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "2f776241-6763-4438-ab2e-d9f39f7a7310",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 384]) torch.Size([2, 384])\n"
     ]
    }
   ],
   "source": [
    "outputs = model(**inputs)\n",
    "\n",
    "start_logits = outputs.start_logits\n",
    "end_logits = outputs.end_logits\n",
    "print(start_logits.shape, end_logits.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "689d23b1-7271-464c-a09f-dc2de6639356",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_ids = inputs.sequence_ids()\n",
    "# Mask everything apart from the tokens of the context\n",
    "mask = [i != 1 for i in sequence_ids]\n",
    "# Unmask the [CLS] token\n",
    "mask[0] = False\n",
    "# Mask all the [PAD] tokens\n",
    "mask = torch.logical_or(torch.tensor(mask)[None], (inputs[\"attention_mask\"] == 0))\n",
    "\n",
    "start_logits[mask] = -10000\n",
    "end_logits[mask] = -10000\n",
    "\n",
    "start_probabilities = torch.nn.functional.softmax(start_logits, dim=-1)\n",
    "end_probabilities = torch.nn.functional.softmax(end_logits, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "58bd7410-e2db-46bb-a91d-54fa03d02808",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 18, 0.33866992592811584), (173, 184, 0.9714871048927307)]\n"
     ]
    }
   ],
   "source": [
    "candidates = []\n",
    "for start_probs, end_probs in zip(start_probabilities, end_probabilities):\n",
    "    scores = start_probs[:, None] * end_probs[None, :]\n",
    "    idx = torch.triu(scores).argmax().item()\n",
    "\n",
    "    start_idx = idx // scores.shape[1]\n",
    "    end_idx = idx % scores.shape[1]\n",
    "    score = scores[start_idx, end_idx].item()\n",
    "    candidates.append((start_idx, end_idx, score))\n",
    "\n",
    "print(candidates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "0767fc29-b37d-4bbd-a426-f7395a87faa8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'answer': '\\n🤗 Transformers: State of the Art NLP', 'start': 0, 'end': 37, 'score': 0.33866992592811584}\n",
      "{'answer': 'Jax, PyTorch and TensorFlow', 'start': 1892, 'end': 1919, 'score': 0.9714871048927307}\n"
     ]
    }
   ],
   "source": [
    "for candidate, offset in zip(candidates, offsets):\n",
    "    start_token, end_token, score = candidate\n",
    "    start_char, _ = offset[start_token]\n",
    "    _, end_char = offset[end_token]\n",
    "    answer = long_context[start_char:end_char]\n",
    "    result = {\"answer\": answer, \"start\": start_char, \"end\": end_char, \"score\": score}\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f936260-a1a0-48b8-b943-9dd621029e95",
   "metadata": {},
   "source": [
    "## Normalization\n",
    "\n",
    "Preprocess the input to smooth the results. Frequent normalization include removing accents/fonts/multiple whitespaces, applying a `.lowercase()`, unicode standardisation (e.g. NFC, NFD, NFKC, NFKD) ...\n",
    "\n",
    "Note: Normalization encure risks of altering the original meaning (e.g. `un père indigné` -> `un pere indigne`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "09cb9432-e091-4920-a97e-418f98bd384a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original='Héllò hôw are ü?' normalized='hello how are u?'\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "original = \"Héllò hôw are ü?\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "normalized = tokenizer.backend_tokenizer.normalizer.normalize_str(original)\n",
    "print(f'{original=} {normalized=}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d3729c1-4fb4-45e1-b343-995227bc4d86",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Pre-Tokenizer\n",
    "Split the input into chunks based on some rules (e.g. split on whitespace, or punctuation, transform whitespaces or discard ...).\n",
    "\n",
    "Note: Some Pre-Tokenizer are lossy (e.g. BERT discards whitespaces, so it's not possible to recronstruct the original input)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "1a987fba-4dd3-4bf1-9e8c-b0c596a75d98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Hello', (0, 5)),\n",
       " (',', (5, 6)),\n",
       " ('how', (7, 10)),\n",
       " ('are', (11, 14)),\n",
       " ('you', (16, 19)),\n",
       " ('?', (19, 20))]"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "tokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str(\"Hello, how are  you?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "5c2a9051-408a-48a4-9160-c60fb6c5974e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Hello', (0, 5)),\n",
       " (',', (5, 6)),\n",
       " ('Ġhow', (6, 10)),\n",
       " ('Ġare', (10, 14)),\n",
       " ('Ġ', (14, 15)),\n",
       " ('Ġyou', (15, 19)),\n",
       " ('?', (19, 20))]"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "tokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str(\"Hello, how are  you?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "c9dd74da-d887-4454-ac76-4290eda5735a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('▁Hello,', (0, 6)),\n",
       " ('▁how', (7, 10)),\n",
       " ('▁are', (11, 14)),\n",
       " ('▁you?', (16, 20))]"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"t5-small\")\n",
    "tokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str(\"Hello, how are  you?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "215584cb-d57d-4c1f-afc5-5b74f9304ba7",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Popular Tokenizer Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e6aa373-a915-4f9e-a8ad-a37658c86c08",
   "metadata": {},
   "source": [
    "### BPE\n",
    "Tokenize by letters and make a frequency of pair queue. Merge the most common pair, replace the pairs with the new merged token and start again until we reach the desired number of tokens."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1faf1d91-e908-4ed2-b9d5-0ded07e6708c",
   "metadata": {},
   "source": [
    "### WordPiece\n",
    "Tokenize by letters, adding a prefix of `##` for letters that do not start a word (e.g. `hug` -> `['h', '##u', '##g']`). Similar to BPE make all the pairs. And compute a score for each pair:\n",
    "\n",
    "$score = {freq\\_of\\_pair \\over {freq\\_of\\_first\\_elem\\ \\ *\\ \\ freq\\_of\\_second\\_elem}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88f0420b-6382-4cff-a81d-915b7d900b91",
   "metadata": {},
   "source": [
    "### Unigram\n",
    "Start with a very large vocabulary and prune it until we reached desired vocabulary size. At each step we compute the unigram loss to decide what element to prune.\n",
    "\n",
    "$loss = \\sum {freq} * (-log(P(word)))$\n",
    "\n",
    "There is an effictient algorithm for it called [Viterbi Algorithm](https://en.wikipedia.org/wiki/Viterbi_algorithm)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9880d27-06f2-4022-813b-b2c07dd4ed0e",
   "metadata": {},
   "source": [
    "## Tokenizer from scratch\n",
    "Recreate BERT."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38f5fbeb-a6f1-43ed-a0e0-be7d3d558eca",
   "metadata": {},
   "source": [
    "### Create a Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "946474e4-1c13-43ea-a7e4-c809d569b747",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"wikitext\", name='wikitext-2-raw-v1', split='train')\n",
    "\n",
    "def get_training_corpus():\n",
    "    for i in range(0, len(dataset), 1000):\n",
    "        yield dataset[i : i + 1000]['text']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be88081a-5780-419e-8bd9-08396b29b619",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0d70e66b-b193-4461-8932-8b78ee1b5fb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import decoders, models, normalizers, pre_tokenizers, processors, trainers, Tokenizer\n",
    "\n",
    "tokenizer = Tokenizer(models.WordPiece(unk_token=\"[UNK]\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba7e8981-3264-4cd0-ad88-95d3d81aab66",
   "metadata": {},
   "source": [
    "### Normalizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3757529a-bde7-4b24-8819-f5416209e807",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello how are u?\n"
     ]
    }
   ],
   "source": [
    "tokenizer.normalizer = normalizers.Sequence([normalizers.NFD(), normalizers.Lowercase(), normalizers.StripAccents()])\n",
    "print(tokenizer.normalizer.normalize_str(\"Héllò hôw are ü?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80869f08-7448-4106-bc27-25701c4a13c4",
   "metadata": {},
   "source": [
    "### Pre-Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5a3bdbc0-c9a7-4ab5-86f3-dce3084071f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Let', (0, 3)),\n",
       " (\"'\", (3, 4)),\n",
       " ('s', (4, 5)),\n",
       " ('test', (6, 10)),\n",
       " ('my', (11, 13)),\n",
       " ('pre', (14, 17)),\n",
       " ('-', (17, 18)),\n",
       " ('tokenizer', (18, 27)),\n",
       " ('.', (27, 28))]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pre_tokenizer = pre_tokenizers.Sequence([pre_tokenizers.WhitespaceSplit(), pre_tokenizers.Punctuation()])\n",
    "pre_tokenizer.pre_tokenize_str(\"Let's test my pre-tokenizer.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "529b00ae-2395-426f-ab94-bcde15b7a29e",
   "metadata": {},
   "source": [
    "### Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "15f9c809-98d7-4e6b-bd40-2d136c170bfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "special_tokens = [\"[UNK]\", \"[PAD]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"]\n",
    "trainer = trainers.WordPieceTrainer(vocab_size=25000, special_tokens=special_tokens)\n",
    "tokenizer.train_from_iterator(get_training_corpus(), trainer=trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "85aa931c-aed9-4883-b3ed-9739f78901fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['l', '##et', \"##'s t\", '##est ', '##this ', '##to', '##ken', '##iz', '##er', '##.']\n"
     ]
    }
   ],
   "source": [
    "encoding = tokenizer.encode(\"Let's test this tokenizer.\")\n",
    "print(encoding.tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9155d670-1a05-42b9-ba44-c8bfb92c9dc9",
   "metadata": {},
   "source": [
    "### Post-Processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1a26aa06-ef39-418c-814b-ceaa2ea7a191",
   "metadata": {},
   "outputs": [],
   "source": [
    "cls_token_id = tokenizer.token_to_id('[CLS]')\n",
    "sep_token_id = tokenizer.token_to_id('[SEP]')\n",
    "\n",
    "tokenizer.post_processor = processors.TemplateProcessing(\n",
    "    single=f'[CLS]:0 $A:0 [SEP]:0',\n",
    "    pair=f'[CLS]:0 $A:0 [SEP]:0 $B:1 [SEP]:1',\n",
    "    special_tokens=[('[CLS]', cls_token_id), ('[SEP]', sep_token_id)],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b16cae17-69cd-42fe-b1ea-49c1156ad938",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'l', '##et', \"##'s t\", '##est ', '##this ', '##to', '##ken', '##iz', '##er', '##.', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "encoding = tokenizer.encode(\"Let's test this tokenizer.\")\n",
    "print(encoding.tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1388e262-d4e7-410d-877c-6aff7ab3732f",
   "metadata": {},
   "source": [
    "### Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9bee12fb-8bd0-4b6e-a688-deff8d7700ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.decoder = decoders.WordPiece(prefix=\"##\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8dca8dbf-a05a-4646-97fc-1fe6cc6779fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"let's test this tokenizer.\""
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(encoding.ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df66f8ac-3e42-44c2-922a-8c1e34277e54",
   "metadata": {},
   "source": [
    "### Save / Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "12e958ee-9f84-4de4-b50b-6578652e106a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.save(\"tokenizer.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0e05a2a1-15e0-4757-acbb-4d4a7cd15fca",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_tokenizer = Tokenizer.from_file(\"tokenizer.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeedef24-6ea1-4997-b9e3-e6ff4a756113",
   "metadata": {},
   "source": [
    "### Make it part of the Fast Tokenizer library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5de436de-2cb7-4d78-bff7-036db70c03bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import PreTrainedTokenizerFast\n",
    "\n",
    "wrapped_tokenizer = PreTrainedTokenizerFast(\n",
    "    tokenizer_object=tokenizer,\n",
    "    # tokenizer_file=\"tokenizer.json\", # You can load from the tokenizer file, alternatively\n",
    "    unk_token=\"[UNK]\",\n",
    "    pad_token=\"[PAD]\",\n",
    "    cls_token=\"[CLS]\",\n",
    "    sep_token=\"[SEP]\",\n",
    "    mask_token=\"[MASK]\",\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_hf_nlp",
   "language": "python",
   "name": "venv_hf_nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
