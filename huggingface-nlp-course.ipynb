{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bad813e3-69ce-45e8-b4ee-dfd6eec7873f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Huggingface - NLP Crourse - Chapter 1: Transformer Models\n",
    "\n",
    "I'll be following along with: https://huggingface.co/learn/nlp-course/chapter1/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca8c1e08-9ad0-4c20-8e00-83d8cd14bf5c",
   "metadata": {},
   "source": [
    "## Demo of `pipeline()` capabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ac46acd3-a7ea-4608-843e-d177d6031533",
   "metadata": {},
   "outputs": [],
   "source": [
    "# requirements\n",
    "# ---\n",
    "# !pip install transformers[sentencepiece]\n",
    "# !pip install sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a740f6b3-b653-4b1f-9011-c92837ed057a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f271817c-7c6a-4838-9cb4-9545fc9680b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert-base-uncased-finetuned-sst-2-english and revision af0f99b (https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'label': 'POSITIVE', 'score': 0.9598049521446228},\n",
       " {'label': 'NEGATIVE', 'score': 0.9994558691978455}]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier = pipeline(\"sentiment-analysis\")\n",
    "classifier([\n",
    "  \"I've been waiting for a HuggingFace course my whole life.\",\n",
    "  \"I hate this so much!\"\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "00fb0cbb-f1f0-4278-a1c8-fca492b0af0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to facebook/bart-large-mnli and revision c626438 (https://huggingface.co/facebook/bart-large-mnli).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'sequence': 'This is a course about the Transformers library',\n",
       " 'labels': ['education', 'business', 'politics'],\n",
       " 'scores': [0.8445982336997986, 0.1119748130440712, 0.043426983058452606]}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier = pipeline(\"zero-shot-classification\")\n",
    "classifier(\n",
    "    \"This is a course about the Transformers library\",\n",
    "    candidate_labels=[\"education\", \"politics\", \"business\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "496c7886-37e1-43d6-80e9-16556558fedb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to gpt2 and revision 6c0e608 (https://huggingface.co/gpt2).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'In this course, we will teach you how to develop a fully automated production model using an open source software framework, which comes with all the tools needed to efficiently work with your environment.\\n\\nI will show you two ways you can configure how you'}]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generator = pipeline(\"text-generation\")\n",
    "generator(\"In this course, we will teach you how to\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0c5a5e0a-868c-422f-9d0d-3c337eb1f2ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'In this course, we will teach you how to use an HTML5-based app on HTML5-powered tablets. In this course, we will'},\n",
       " {'generated_text': 'In this course, we will teach you how to start DGX.'}]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generator = pipeline(\"text-generation\", model=\"distilgpt2\")\n",
    "generator(\n",
    "    \"In this course, we will teach you how to\",\n",
    "    max_length=30,\n",
    "    num_return_sequences=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "18e184e6-8b7a-44e5-accd-e2eb095130fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilroberta-base and revision ec58a5b (https://huggingface.co/distilroberta-base).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Some weights of the model checkpoint at distilroberta-base were not used when initializing RobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'score': 0.1961977779865265,\n",
       "  'token': 30412,\n",
       "  'token_str': ' mathematical',\n",
       "  'sequence': 'This course will teach you all about mathematical models.'},\n",
       " {'score': 0.0405273362994194,\n",
       "  'token': 38163,\n",
       "  'token_str': ' computational',\n",
       "  'sequence': 'This course will teach you all about computational models.'}]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unmasker = pipeline(\"fill-mask\")\n",
    "unmasker(\"This course will teach you all about <mask> models.\", top_k=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2aaa94fe-ad62-45f4-953f-aec916266d8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to dbmdz/bert-large-cased-finetuned-conll03-english and revision f2482bf (https://huggingface.co/dbmdz/bert-large-cased-finetuned-conll03-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Some weights of the model checkpoint at dbmdz/bert-large-cased-finetuned-conll03-english were not used when initializing BertForTokenClassification: ['bert.pooler.dense.weight', 'bert.pooler.dense.bias']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "C:\\Users\\p\\Desktop\\_ML\\huggingface-nlp-course\\venv\\Lib\\site-packages\\transformers\\pipelines\\token_classification.py:169: UserWarning: `grouped_entities` is deprecated and will be removed in version v5.0.0, defaulted to `aggregation_strategy=\"AggregationStrategy.SIMPLE\"` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'entity_group': 'PER',\n",
       "  'score': 0.9981694,\n",
       "  'word': 'Sylvain',\n",
       "  'start': 11,\n",
       "  'end': 18},\n",
       " {'entity_group': 'ORG',\n",
       "  'score': 0.9796019,\n",
       "  'word': 'Hugging Face',\n",
       "  'start': 33,\n",
       "  'end': 45},\n",
       " {'entity_group': 'LOC',\n",
       "  'score': 0.9932106,\n",
       "  'word': 'Brooklyn',\n",
       "  'start': 49,\n",
       "  'end': 57}]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ner = pipeline(\"ner\", grouped_entities=True)\n",
    "ner(\"My name is Sylvain and I work at Hugging Face in Brooklyn.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0677841e-1b1f-40fd-bbd5-9863998517ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert-base-cased-distilled-squad and revision 626af31 (https://huggingface.co/distilbert-base-cased-distilled-squad).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'score': 0.6949770450592041, 'start': 33, 'end': 45, 'answer': 'Hugging Face'}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question_answerer = pipeline(\"question-answering\")\n",
    "question_answerer(\n",
    "    question=\"Where do I work?\",\n",
    "    context=\"My name is Sylvain and I work at Hugging Face in Brooklyn\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8dc08ed2-720f-44b4-a8c3-26cf2175c548",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to sshleifer/distilbart-cnn-12-6 and revision a4f8f3e (https://huggingface.co/sshleifer/distilbart-cnn-12-6).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'summary_text': ' America has changed dramatically during recent years . The number of engineering graduates in the U.S. has declined in traditional engineering disciplines such as mechanical, civil,    electrical, chemical, and aeronautical engineering . Rapidly developing economies such as China and India continue to encourage and advance the teaching of engineering .'}]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summarizer = pipeline(\"summarization\")\n",
    "summarizer(\n",
    "    \"\"\"\n",
    "    America has changed dramatically during recent years. Not only has the number of \n",
    "    graduates in traditional engineering disciplines such as mechanical, civil, \n",
    "    electrical, chemical, and aeronautical engineering declined, but in most of \n",
    "    the premier American universities engineering curricula now concentrate on \n",
    "    and encourage largely the study of engineering science. As a result, there \n",
    "    are declining offerings in engineering subjects dealing with infrastructure, \n",
    "    the environment, and related issues, and greater concentration on high \n",
    "    technology subjects, largely supporting increasingly complex scientific \n",
    "    developments. While the latter is important, it should not be at the expense \n",
    "    of more traditional engineering.\n",
    "\n",
    "    Rapidly developing economies such as China and India, as well as other \n",
    "    industrial countries in Europe and Asia, continue to encourage and advance \n",
    "    the teaching of engineering. Both China and India, respectively, graduate \n",
    "    six and eight times as many traditional engineers as does the United States. \n",
    "    Other industrial countries at minimum maintain their output, while America \n",
    "    suffers an increasingly serious decline in the number of engineering graduates \n",
    "    and a lack of well-educated engineers.\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9fd924e8-54f4-47b9-8f68-c0b2146b515f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\p\\Desktop\\_ML\\huggingface-nlp-course\\venv\\Lib\\site-packages\\transformers\\models\\marian\\tokenization_marian.py:194: UserWarning: Recommended: pip install sacremoses.\n",
      "  warnings.warn(\"Recommended: pip install sacremoses.\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'translation_text': 'This course is produced by Hugging Face.'}]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translator = pipeline(\"translation\", model=\"Helsinki-NLP/opus-mt-fr-en\")\n",
    "translator(\"Ce cours est produit par Hugging Face.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d24630b6-a2f2-4a54-882b-626d961222b5",
   "metadata": {},
   "source": [
    "## Bias and limitations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1d656737-f931-447d-9d2d-4a01118450d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'bert.pooler.dense.weight', 'bert.pooler.dense.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['carpenter', 'lawyer', 'farmer', 'businessman', 'doctor']\n",
      "['nurse', 'maid', 'teacher', 'waitress', 'prostitute']\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "unmasker = pipeline(\"fill-mask\", model=\"bert-base-uncased\")\n",
    "result = unmasker(\"This man works as a [MASK].\")\n",
    "print([r[\"token_str\"] for r in result])\n",
    "\n",
    "result = unmasker(\"This woman works as a [MASK].\")\n",
    "print([r[\"token_str\"] for r in result])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ddcd1e4-888b-41a0-896d-cbe829b1d430",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Huggingface - NLP Crourse - Chapter 2: Using 🤗 Transformers\n",
    "I'll be following along with: https://huggingface.co/learn/nlp-course/chapter2/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fcacff8-6e68-4a39-b01c-4ce2574940e2",
   "metadata": {},
   "source": [
    "## Re-creating the `pipeline()`\n",
    "We are going to replicate the functionality of this `pipeline()` step by step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "afe6966b-ca61-4985-974f-65a1cf3a8a77",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert-base-uncased-finetuned-sst-2-english and revision af0f99b (https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'label': 'POSITIVE', 'score': 0.9598049521446228},\n",
       " {'label': 'NEGATIVE', 'score': 0.9994558691978455}]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "classifier = pipeline(\"sentiment-analysis\")\n",
    "classifier(\n",
    "    [\n",
    "        \"I've been waiting for a HuggingFace course my whole life.\",\n",
    "        \"I hate this so much!\",\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a946538a-3267-44e4-bc9a-1053df204cf2",
   "metadata": {},
   "source": [
    "### 1- Tokenizer\n",
    "`pipeline()` pre-process the inputs into tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a6553044-e121-442f-b81c-aad8387050bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1c9ea5c3-c717-43fd-8512-97711260979f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[  101,  1045,  1005,  2310,  2042,  3403,  2005,  1037, 17662, 12172,\n",
      "          2607,  2026,  2878,  2166,  1012,   102],\n",
      "        [  101,  1045,  5223,  2023,  2061,  2172,   999,   102,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0]])}\n"
     ]
    }
   ],
   "source": [
    "raw_inputs = [\n",
    "    \"I've been waiting for a HuggingFace course my whole life.\",\n",
    "    \"I hate this so much!\",\n",
    "]\n",
    "inputs = tokenizer(raw_inputs, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "print(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0725f065-b476-4c87-9a62-8709cbf46301",
   "metadata": {},
   "source": [
    "### 2- Model\n",
    "The tokens are passed to a pre-trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9043d28b-2909-4192-9f1f-b595bab7289d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel\n",
    "\n",
    "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "model = AutoModel.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b7e68517-276c-40ca-866d-037b920f6496",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 16, 768])\n"
     ]
    }
   ],
   "source": [
    "outputs = model(**inputs)\n",
    "print(outputs.last_hidden_state.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd179a86-493f-4983-a72b-7e9ba3919a15",
   "metadata": {},
   "source": [
    "It produces a tensor `torch.Size([2, 16, 768])` or `batch_size = 2`, `sequence_length = 16`, and `hidden_size = 768` because the `AutoModel` doesn't include the head of the model.\n",
    "\n",
    "To have the output of the head we use `AutoModelForSequenceClassification` instead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "15cd0093-5f91-4217-806e-c3b32c677617",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
    "outputs = model(**inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fafc370a-caf2-440f-b595-d808d00c5b48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.5607,  1.6123],\n",
      "        [ 4.1692, -3.3464]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(outputs.logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6732d817-18dd-453e-b0cb-c56ce2c81649",
   "metadata": {},
   "source": [
    "The output tensor of size [2, 2] is not yet probabilities (they do not sum to 1), because the model returns logits."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eccc2c34-5b90-461b-af6f-87ed6ab2067f",
   "metadata": {},
   "source": [
    "### 3- Postprocessing\n",
    "Apply `softmax()` to logits to normalize the probabilities to 1 and retrieve the labels in `model.config.id2label`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "52af8b9d-6d83-4180-bbae-2b52c4a8e779",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[4.0195e-02, 9.5980e-01],\n",
      "        [9.9946e-01, 5.4418e-04]], grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "43142a2e-0b25-4456-a55a-ee79aed895c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'NEGATIVE', 1: 'POSITIVE'}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.config.id2label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d3087c56-41aa-4e13-ab2c-43a8ecde3cf8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['NEGATIVE : 4.02%', 'POSITIVE : 95.98%'],\n",
       " ['NEGATIVE : 99.95%', 'POSITIVE : 0.05%']]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[[f'{model.config.id2label[i]} : {pred:.2%}' for i, pred in enumerate(prediction)] for prediction in predictions]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52c692c5-338b-4e66-b6a7-e22e0d4c3940",
   "metadata": {},
   "source": [
    "## Model and a checkpoint\n",
    "Instantiating a model with no checkpoint makes a model with random weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "016b7ec8-e20c-4290-a5c4-97228e5fae15",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertConfig, BertModel\n",
    "\n",
    "# Building the config\n",
    "config = BertConfig()\n",
    "\n",
    "# Building the model from the config\n",
    "model = BertModel(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "411a2e98-18c0-41c0-b020-b2eaa1f89413",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.32.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6857774-dc0c-4d0a-b7fc-32d181639572",
   "metadata": {},
   "source": [
    "Let's load a pre-trained model from a checkpoint instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "72aa557e-b39f-4a6b-967b-b3bcff678465",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertModel\n",
    "\n",
    "model = BertModel.from_pretrained(\"bert-base-cased\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44a62c61-c5d8-4f08-a369-b8eaf26d99c9",
   "metadata": {},
   "source": [
    "Models can be saved to disk using `.save_pretrained()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d1ac51d0-c44a-403a-8fea-a3ae7f0c934e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(\"directory_on_my_computer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6970ac13-f144-4113-80c6-1cd94855f6d3",
   "metadata": {},
   "source": [
    "## Tokenizer\n",
    "Models can only work with number, so inputs are converted to numbers before being fed to the model. The 3 main tokenization algorithms are:\n",
    "- Word based\n",
    "- Character based\n",
    "- Subword based"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fc9e732-af9d-42fc-86eb-56ec34d007fd",
   "metadata": {},
   "source": [
    "### Word based tokenization\n",
    "Split text into words (by splitting on whitespace) and words are then converted to a unique id.\n",
    "\n",
    "e.g.: `'I like turtle'` -> `['I', 'like', 'turtle']` -> `[17, 494, 237]`\n",
    "\n",
    "Each word carry a lot of context and semantic meaning, but it has limitations, `dog` is almost like `dogs` but they will be treated completely differently. We also need an id for each word in the vocabulary which create a very large mapping and makes models bloated. Every new word will be treated as `out_of_vocabulary` which will also lead to a loss of information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c826a099-96b4-4324-9d08-84a0c2d880d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'like', 'turtle']\n",
      "[17, 494, 237]\n"
     ]
    }
   ],
   "source": [
    "vocabulary = {\n",
    "  '[OUT OF VOCABULARY]': 0,\n",
    "  'I': 17,\n",
    "  'turtle': 237,\n",
    "  'like': 494,\n",
    "}\n",
    "text = 'I like turtle'\n",
    "tokens = text.split()\n",
    "print(tokens)\n",
    "numerical_tokens = [vocabulary.get(t, 0) for t in tokens]\n",
    "print(numerical_tokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9e53d93-3d0d-4f64-a8b3-cda40f67e70d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Character based tokenization\n",
    "Split text into characters.\n",
    "\n",
    "e.g.: `'I like turtle'` -> `['I', ' ', 'l', 'i', 'k', 'e', ' ', 't', 'u', 'r', 't', 'l', 'e']` -> `[73, 32, 108, 105, 107, 101, 32, 116, 117, 114, 116, 108, 101]`\n",
    "\n",
    "The vocabulary is much smaller and cover the entire possible words spelled in your charset. But each token has a weaker meaning and hold less information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "119bd3f6-6941-45bf-842b-fb82e1f584e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', ' ', 'l', 'i', 'k', 'e', ' ', 't', 'u', 'r', 't', 'l', 'e']\n",
      "[73, 32, 108, 105, 107, 101, 32, 116, 117, 114, 116, 108, 101]\n"
     ]
    }
   ],
   "source": [
    "vocabulary = {c: i for i, c in enumerate([chr(n) for n in range(255)])}\n",
    "text = 'I like turtle'\n",
    "tokens = list(text)\n",
    "print(tokens)\n",
    "numerical_tokens = [vocabulary.get(t, 0) for t in tokens]\n",
    "print(numerical_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58018189-533d-4933-8f03-3e08f6f25152",
   "metadata": {},
   "source": [
    "## Subword based tokenization\n",
    "It's a compromise between word and char tokenizer.\n",
    "\n",
    "Frequent words are not split, while rare words are decomposed into meaningful subwords.\n",
    "\n",
    "e.g.:\n",
    "- `dog` -> `dog\\w`\n",
    "- `dogs` -> [`dog`, `s\\w`]\n",
    "- `tokenization` -> [`token`, `##ization`]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3b57b797-138e-4354-bbf3-9b0c06362bb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-cased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f24c90e7-5c72-4baf-a38a-130286a07b64",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "12104d3d-d44d-477d-85b1-ee5cd6b47c45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Using', 'a', 'Trans', '##former', 'network', 'is', 'simple']\n",
      "[7993, 170, 13809, 23763, 2443, 1110, 3014]\n",
      "Using a transformer network is simple\n"
     ]
    }
   ],
   "source": [
    "sequence = \"Using a Transformer network is simple\"\n",
    "\n",
    "# tokenize step by step\n",
    "tokens = tokenizer.tokenize(sequence)\n",
    "print(tokens)\n",
    "ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "print(ids)\n",
    "\n",
    "# or use the equivalent\n",
    "tokenizer(sequence)\n",
    "\n",
    "# decoding\n",
    "decoded_string = tokenizer.decode([7993, 170, 11303, 1200, 2443, 1110, 3014])\n",
    "print(decoded_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3443897-06cd-46e2-8972-ae185e34cf49",
   "metadata": {},
   "source": [
    "## Tokenizing multiple inputs at once\n",
    "When tokenizing multiple inputs we have to make sure they are the same size as tensors need to have regular dimensions. So the inputs are padded up to a certain size. To match that we pass a mask for the attention layer so the padding tokens are ignored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "70657951-e3d1-4adc-afa9-9fffe0c91d73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input IDs: tensor([[ 1045,  1005,  2310,  2042,  3403,  2005,  1037, 17662, 12172,  2607,\n",
      "          2026,  2878,  2166,  1012]])\n",
      "Logits: tensor([[-2.7276,  2.8789]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
    "\n",
    "sequence = \"I've been waiting for a HuggingFace course my whole life.\"\n",
    "\n",
    "tokens = tokenizer.tokenize(sequence)\n",
    "ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "\n",
    "input_ids = torch.tensor([ids])\n",
    "print(\"Input IDs:\", input_ids)\n",
    "\n",
    "output = model(input_ids)\n",
    "print(\"Logits:\", output.logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3443c26-6add-4907-ab6b-21ad69b94108",
   "metadata": {},
   "source": [
    "For batched requests we need to pad the inputs to be the same size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "87ac0dfb-a901-4286-ad75-e1fe9c648564",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.5694, -1.3895],\n",
      "        [ 0.5803, -0.4125]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "batched_ids = [\n",
    "    [200, 200, 200],\n",
    "    [200, 200, tokenizer.pad_token_id],\n",
    "]\n",
    "\n",
    "attention_mask = [\n",
    "    [1, 1, 1],\n",
    "    [1, 1, 0],\n",
    "]\n",
    "\n",
    "outputs = model(torch.tensor(batched_ids), attention_mask=torch.tensor(attention_mask))\n",
    "print(outputs.logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ed19669-7901-400a-8989-6b0aa31c70c7",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Huggingface - NLP Crourse - Chapter 3: Fine-Tuning a Pretrained Model\n",
    "I'll be following along with: https://huggingface.co/learn/nlp-course/chapter3/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d34f1399-2664-4001-a4ed-d14ea24dff90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install datasets evaluate transformers[torch] scipy sklearn scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34f56ada-e4c5-4e4c-96ac-cab45a9987c0",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "The dataset is stored on disk using [Apache Arrow](https://arrow.apache.org/docs/python/dataset.html). Only requested rows are loaded in memory. So we can manipulate big datasets without going OOM."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91c063e0-c18a-4b24-bff5-698990b28910",
   "metadata": {},
   "source": [
    "## Load the dataset\n",
    "Here we are using a dataset comming from a [Microsoft paper](https://aclanthology.org/I05-5002.pdf), it provides a corpus for \"Sentential Paraphrases\" (aka. equivalent sentences).\n",
    "\n",
    "Additional datasets can be found at: https://huggingface.co/datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9dd0deca-c749-4dc2-8e92-6779a0a3e9ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['sentence1', 'sentence2', 'label', 'idx'],\n",
       "        num_rows: 3668\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['sentence1', 'sentence2', 'label', 'idx'],\n",
       "        num_rows: 408\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['sentence1', 'sentence2', 'label', 'idx'],\n",
       "        num_rows: 1725\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "raw_datasets = load_dataset(\"glue\", \"mrpc\")\n",
    "raw_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9eee00ff-f349-475d-bc1a-89cd4daed8fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sentence1': 'Amrozi accused his brother , whom he called \" the witness \" , of deliberately distorting his evidence .',\n",
       " 'sentence2': 'Referring to him as only \" the witness \" , Amrozi accused his brother of deliberately distorting his evidence .',\n",
       " 'label': 1,\n",
       " 'idx': 0}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_train_dataset = raw_datasets[\"train\"]\n",
    "raw_train_dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6114601d-9069-480f-9b1d-f2fbc1195688",
   "metadata": {},
   "source": [
    " `.features['label']` provides the correspondance between the label id and the human meaning. Here `id 0 = 'not_equivalent'` and `id 1 = 'equivalent'`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1438b5d2-a8cd-4114-9bcc-17ce00b6f41b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sentence1': Value(dtype='string', id=None),\n",
       " 'sentence2': Value(dtype='string', id=None),\n",
       " 'label': ClassLabel(names=['not_equivalent', 'equivalent'], id=None),\n",
       " 'idx': Value(dtype='int32', id=None)}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_train_dataset.features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f5db3da-2ec5-4886-9a63-609f3189f369",
   "metadata": {},
   "source": [
    "## Tokenize the entries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc725cc9-9514-48a5-83fb-08b941696793",
   "metadata": {},
   "source": [
    "### With fixed padding\n",
    "This is the easiest solution. Pad the entire dataset to a fixed size (here 128) and this is fast on TPU.\n",
    "\n",
    "Note: Dynamic padding can provide a speedup on CPU/GPU for the batches with only smaller entries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "53208af1-318d-4e79-9d27-d3210be42341",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 3668\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 408\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 1725\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "checkpoint = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "\n",
    "def tokenize_function(example):\n",
    "    return tokenizer(example[\"sentence1\"], example[\"sentence2\"], padding='max_length', truncation=True, max_length=128)\n",
    "\n",
    "tokenized_datasets = raw_datasets.map(tokenize_function, batched=True)\n",
    "tokenized_datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62e3e5dd-2f1e-43ff-8784-e51036b6f965",
   "metadata": {},
   "source": [
    "### With dynamic padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "655982c4-c5fd-49d1-9819-1d86985a3d3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, DataCollatorWithPadding\n",
    "\n",
    "checkpoint = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "\n",
    "def tokenize_function(example):\n",
    "    return tokenizer(example[\"sentence1\"], example[\"sentence2\"], truncation=True)\n",
    "\n",
    "tokenized_datasets = raw_datasets.map(tokenize_function, batched=True)\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26ac894d-7591-472d-ad5d-4d887b9ec311",
   "metadata": {},
   "source": [
    "### Cleanup the data\n",
    "Remove unused columns and rename `label` to `labels` to make the huggingface model happy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "11d73e41-9043-43ba-a031-fd14abded5c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['labels', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "    num_rows: 3668\n",
       "})"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_datasets = tokenized_datasets.remove_columns(['idx', 'sentence1', 'sentence2'])\n",
    "tokenized_datasets = tokenized_datasets.rename_column('label', 'labels')\n",
    "tokenized_datasets = tokenized_datasets.with_format('torch')\n",
    "tokenized_datasets['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a494dd23-793f-4103-aee5-95c19abf9628",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: we can get a smaller subset of the data by using `.select()`\n",
    "# small_train_dataset = tokenized_datasets['train'].select(range(100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a3c1d72-adaa-4612-93e9-7cc5d9a9b8bb",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f3ac7d5c-f1a9-4c2c-8ade-d2318d740e77",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "from transformers import Trainer\n",
    "\n",
    "training_args = TrainingArguments(\"test-trainer\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)\n",
    "trainer = Trainer(\n",
    "    model,\n",
    "    training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "97509826-6ce8-4558-a38d-b5abe0155e11",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1377' max='1377' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1377/1377 01:19, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.597400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.476100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1377, training_loss=0.490657354838275, metrics={'train_runtime': 81.1125, 'train_samples_per_second': 135.663, 'train_steps_per_second': 16.976, 'total_flos': 405324636337200.0, 'train_loss': 0.490657354838275, 'epoch': 3.0})"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "442bc352-2874-49be-9d26-4d952d1ac2a9",
   "metadata": {},
   "source": [
    "### Evaluate the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "4e32eeb0-e1c8-4358-b0b1-ad40042eca8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(408, 2) (408,)\n"
     ]
    }
   ],
   "source": [
    "predictions = trainer.predict(tokenized_datasets[\"validation\"])\n",
    "print(predictions.predictions.shape, predictions.label_ids.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "668bba8d-0da9-43aa-990c-d416b05945c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.8308823529411765, 'f1': 0.881646655231561}"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import evaluate\n",
    "\n",
    "preds = np.argmax(predictions.predictions, axis=-1)\n",
    "metric = evaluate.load(\"glue\", \"mrpc\")\n",
    "metric.compute(predictions=preds, references=predictions.label_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30e5b5d9-1e40-43ef-baff-3e7cf9950048",
   "metadata": {},
   "source": [
    "### Train + Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "f6d29e1e-fe2e-4144-96a2-9ddcb8d86f27",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1377' max='1377' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1377/1377 01:20, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.343839</td>\n",
       "      <td>0.852941</td>\n",
       "      <td>0.896552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.492500</td>\n",
       "      <td>0.607140</td>\n",
       "      <td>0.848039</td>\n",
       "      <td>0.897010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.289700</td>\n",
       "      <td>0.715746</td>\n",
       "      <td>0.857843</td>\n",
       "      <td>0.899306</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1377, training_loss=0.32119023773924715, metrics={'train_runtime': 80.515, 'train_samples_per_second': 136.67, 'train_steps_per_second': 17.102, 'total_flos': 405540469624800.0, 'train_loss': 0.32119023773924715, 'epoch': 3.0})"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def compute_metrics(eval_preds):\n",
    "    metric = evaluate.load(\"glue\", \"mrpc\")\n",
    "    logits, labels = eval_preds\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels)\n",
    "\n",
    "training_args = TrainingArguments(\"test-trainer\", evaluation_strategy=\"epoch\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model,\n",
    "    training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98c7a7aa-29e3-4da1-93bb-0d6a7e8b9720",
   "metadata": {},
   "source": [
    "## Train by hand with PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "3664d1e9-4f14-4217-bf7f-20aff7a840fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\p\\Desktop\\_ML\\huggingface-nlp-course\\venv\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65077a413fdf48f98b8ad74b5ce06f67",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1377 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from accelerate import Accelerator\n",
    "from transformers import AdamW, AutoModelForSequenceClassification, get_scheduler\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    tokenized_datasets[\"train\"], shuffle=True, batch_size=8, collate_fn=data_collator\n",
    ")\n",
    "eval_dataloader = DataLoader(\n",
    "    tokenized_datasets[\"validation\"], batch_size=8, collate_fn=data_collator\n",
    ")\n",
    "\n",
    "accelerator = Accelerator()\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)\n",
    "optimizer = AdamW(model.parameters(), lr=3e-5)\n",
    "\n",
    "train_dl, eval_dl, model, optimizer = accelerator.prepare(\n",
    "    train_dataloader, eval_dataloader, model, optimizer\n",
    ")\n",
    "\n",
    "num_epochs = 3\n",
    "num_training_steps = num_epochs * len(train_dl)\n",
    "lr_scheduler = get_scheduler(\n",
    "    \"linear\",\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=num_training_steps,\n",
    ")\n",
    "\n",
    "progress_bar = tqdm(range(num_training_steps))\n",
    "\n",
    "model.train()\n",
    "for epoch in range(num_epochs):\n",
    "    for batch in train_dl:\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        accelerator.backward(loss)\n",
    "\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "        progress_bar.update(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13171cd1-a386-419c-b26e-e89e4b0bc965",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Huggingface - NLP Crourse - Chapter 4: Sharing Models and Tokenizers\n",
    "I'll be following along with: https://huggingface.co/learn/nlp-course/chapter4/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d546843-8220-4caa-b056-1fd53babb1d5",
   "metadata": {},
   "source": [
    "Browse https://huggingface.co/models for models, filter on task/trainingset/backbone. Use the widget to test the model from the webpage."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "461be445-2862-4e02-b288-265cae22b1ab",
   "metadata": {},
   "source": [
    "# Huggingface - NLP Crourse - Chapter 5: The 🤗 Datasets Library\n",
    "I'll be following along with: https://huggingface.co/learn/nlp-course/chapter5/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bb1b5d5-c9ce-4bb1-833f-07853a2e84a0",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Load a dataset from outside the 🤗 ecosystem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "8ff8f9da-ea2a-487f-846a-3d20112a72aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !wget https://github.com/crux82/squad-it/raw/master/SQuAD_it-train.json.gz\n",
    "# !wget https://github.com/crux82/squad-it/raw/master/SQuAD_it-test.json.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9957426-6551-4558-af4a-eac390668b04",
   "metadata": {},
   "source": [
    "### Read from File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "508bdc58-358d-4688-84e4-f07dd0c13eae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "data_files = {\"train\": \"SQuAD_it-train.json\", \"test\": \"SQuAD_it-test.json\"}\n",
    "squad_it_dataset = load_dataset(\"json\", data_files=data_files, field=\"data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6e4b6ec-aef5-4a09-adf8-2fd71706e5d5",
   "metadata": {},
   "source": [
    "### Read from Compressed File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "b25b5091-7bce-4793-a386-a37e9a1e11f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "data_files = {\"train\": \"SQuAD_it-train.json.gz\", \"test\": \"SQuAD_it-test.json.gz\"}\n",
    "squad_it_dataset = load_dataset(\"json\", data_files=data_files, field=\"data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9d513d3-f8af-4a02-b6f3-a8805d9ebf86",
   "metadata": {},
   "source": [
    "### Read from URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "a1496094-06ea-4d6a-830c-a4be29fad375",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "url = \"https://github.com/crux82/squad-it/raw/master/\"\n",
    "data_files = {\n",
    "    \"train\": url + \"SQuAD_it-train.json.gz\",\n",
    "    \"test\": url + \"SQuAD_it-test.json.gz\",\n",
    "}\n",
    "squad_it_dataset = load_dataset(\"json\", data_files=data_files, field=\"data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3bcb9d9-095b-4303-a0c2-459954d84a72",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Slice and Dice the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e38f74d-988f-43ac-800a-14fa25b039c7",
   "metadata": {},
   "source": [
    "### Shuffle the data\n",
    "This prevent the model from learning artificial ordering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "684538d5-4d9b-46ef-9572-484448e6f6c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['5733be284776f41900661182', '5733be284776f4190066117f', '5733be284776f41900661180', '5733be284776f41900661181', '5733be284776f4190066117e']\n",
      "['5727cc873acd2414000deca9', '5730b096396df919000962a0', '5729125daf94a219006aa029', '5727e9e0ff5b5019007d9852', '5731cca5e17f3d140042240a']\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "squad = load_dataset('squad', split='train')\n",
    "squad_shuffled = squad.shuffle(seed=666)\n",
    "\n",
    "print([s['id'] for s in squad.select(range(5))])\n",
    "print([s['id'] for s in squad_shuffled.select(range(5))])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec8249b9-eeef-4647-b8da-85f574df2c86",
   "metadata": {},
   "source": [
    "### Create a random train/test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "30c63a1b-da32-47dc-9047-3ad2b4e95105",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'title', 'context', 'question', 'answers'],\n",
       "        num_rows: 78839\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'title', 'context', 'question', 'answers'],\n",
       "        num_rows: 8760\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "squad = load_dataset('squad', split='train')\n",
    "dataset = squad.train_test_split(test_size=0.1)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3998242-4e0a-432c-84f2-04e8ccc46e56",
   "metadata": {},
   "source": [
    "### Select specific rows from the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "d7f7e5ae-4f64-4814-ae4d-2fe604ad5b81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['List_of_numbered_streets_in_Manhattan', 'LaserDisc', 'List_of_numbered_streets_in_Manhattan', 'Light-emitting_diode', 'LaserDisc']\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "squad = load_dataset('squad', split='train')\n",
    "squad_filtered = squad.filter(lambda x: x['title'].startswith('L'))\n",
    "print([s['title'] for s in squad_filtered.shuffle().select(range(5))])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e792673b-3764-4117-8d34-1aea0de97ca4",
   "metadata": {},
   "source": [
    "### Flatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "5fd49485-1fb6-40e7-b1c1-615870a8e50f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['id', 'title', 'context', 'question', 'answers'])\n",
      "dict_keys(['id', 'title', 'context', 'question', 'answers.text', 'answers.answer_start'])\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "squad = load_dataset('squad', split='train')\n",
    "print(squad.features.keys())\n",
    "squad_flattened = squad.flatten()\n",
    "print(squad_flattened.features.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9adb2ec-05f0-4548-be9f-ff5db1444e1b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Convert Dataset to Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "317663d4-7f5a-4584-803f-198d39e48dd1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>year</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>language</th>\n",
       "      <th>region</th>\n",
       "      <th>canton</th>\n",
       "      <th>legal area</th>\n",
       "      <th>source_language</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>2000</td>\n",
       "      <td>A.- Der 1955 geborene V._ war seit 1. Septembe...</td>\n",
       "      <td>0</td>\n",
       "      <td>de</td>\n",
       "      <td>Zürich</td>\n",
       "      <td>zh</td>\n",
       "      <td>insurance law</td>\n",
       "      <td>n/a</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  year                                               text  label  \\\n",
       "0   2  2000  A.- Der 1955 geborene V._ war seit 1. Septembe...      0   \n",
       "\n",
       "  language  region canton     legal area source_language  \n",
       "0       de  Zürich     zh  insurance law             n/a  "
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset('swiss_judgment_prediction', 'all', split='train')\n",
    "dataset.set_format('pandas')\n",
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "0cfcbfd7-4428-490c-9906-d1675965f1ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>year</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>language</th>\n",
       "      <th>region</th>\n",
       "      <th>canton</th>\n",
       "      <th>legal area</th>\n",
       "      <th>source_language</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>2000</td>\n",
       "      <td>A.- Der 1955 geborene V._ war seit 1. Septembe...</td>\n",
       "      <td>0</td>\n",
       "      <td>de</td>\n",
       "      <td>Zürich</td>\n",
       "      <td>zh</td>\n",
       "      <td>insurance law</td>\n",
       "      <td>n/a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>2000</td>\n",
       "      <td>Ansprüche nach OHG, hat sich ergeben: A.- X._ ...</td>\n",
       "      <td>1</td>\n",
       "      <td>de</td>\n",
       "      <td>Central Switzerland</td>\n",
       "      <td>lu</td>\n",
       "      <td>public law</td>\n",
       "      <td>n/a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>2000</td>\n",
       "      <td>Art. 4 aBV (Strafverfahren wegen falschen Zeug...</td>\n",
       "      <td>0</td>\n",
       "      <td>de</td>\n",
       "      <td>Northwestern Switzerland</td>\n",
       "      <td>ag</td>\n",
       "      <td>public law</td>\n",
       "      <td>n/a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>2000</td>\n",
       "      <td>Art. 5 Ziff. 1 EMRK (Haftentlassung), hat sich...</td>\n",
       "      <td>1</td>\n",
       "      <td>de</td>\n",
       "      <td>n/a</td>\n",
       "      <td>n/a</td>\n",
       "      <td>public law</td>\n",
       "      <td>n/a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6</td>\n",
       "      <td>2000</td>\n",
       "      <td>Mietvertrag, hat sich ergeben: A.- Die CT Cond...</td>\n",
       "      <td>0</td>\n",
       "      <td>de</td>\n",
       "      <td>n/a</td>\n",
       "      <td>n/a</td>\n",
       "      <td>civil law</td>\n",
       "      <td>n/a</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  year                                               text  label  \\\n",
       "0   2  2000  A.- Der 1955 geborene V._ war seit 1. Septembe...      0   \n",
       "1   3  2000  Ansprüche nach OHG, hat sich ergeben: A.- X._ ...      1   \n",
       "2   4  2000  Art. 4 aBV (Strafverfahren wegen falschen Zeug...      0   \n",
       "3   5  2000  Art. 5 Ziff. 1 EMRK (Haftentlassung), hat sich...      1   \n",
       "4   6  2000  Mietvertrag, hat sich ergeben: A.- Die CT Cond...      0   \n",
       "\n",
       "  language                    region canton     legal area source_language  \n",
       "0       de                    Zürich     zh  insurance law             n/a  \n",
       "1       de       Central Switzerland     lu     public law             n/a  \n",
       "2       de  Northwestern Switzerland     ag     public law             n/a  \n",
       "3       de                       n/a    n/a     public law             n/a  \n",
       "4       de                       n/a    n/a      civil law             n/a  "
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = dataset[:]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "a0bf68ff-52d4-41ad-a1d7-96315c575b84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "region                    language\n",
       "Central Switzerland       de           4778\n",
       "                          it              1\n",
       "Eastern Switzerland       de           5650\n",
       "                          it             57\n",
       "Espace Mittelland         de           5150\n",
       "                          fr           3104\n",
       "                          it              3\n",
       "Federation                de           1011\n",
       "                          fr            227\n",
       "                          it             70\n",
       "Northwestern Switzerland  de           5654\n",
       "                          fr              1\n",
       "Région lémanique          fr          13100\n",
       "                          de            336\n",
       "Ticino                    it           2249\n",
       "                          de              6\n",
       "Zürich                    de           8785\n",
       "                          fr              3\n",
       "n/a                       fr           4744\n",
       "                          de           4088\n",
       "                          it            692\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby('region')['language'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "de500ffb-d8db-43c5-b077-82f7ee6bed18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "legal area\n",
       "public law       15173\n",
       "penal law        11795\n",
       "civil law        11477\n",
       "insurance law    11142\n",
       "social law        9727\n",
       "other              395\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['legal area'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "60ef4f70-10d2-4806-9176-b08f8fbf9c66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# when done, get the format back to default\n",
    "dataset.reset_format()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f42637c-17bf-49ec-b2fd-94295a818c27",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Saving/Reload a dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "3988b9b8-e6c8-4213-bd21-f78331910a1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset('allocine')\n",
    "# print(ds.cache_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aff427da-27bd-4a4b-b39e-5b278625eb77",
   "metadata": {},
   "source": [
    "#### Apache Arrow\n",
    "Default efficient format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "1ad7a420-6205-455a-90a0-1e8b2766953a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14ba50312b2e408a92d1374ec6597696",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/160000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b41d8bc130c4255958f6a85bc99e2ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/20000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ceefd45306ee4d4f9479d308a32dfad0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/20000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ds.save_to_disk('local-allocine')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "a96c13a0-d76f-42cf-a55b-b6cc944a7457",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_from_disk\n",
    "\n",
    "arrow_ds = load_from_disk('local-allocine')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58118760-bea9-4e6d-9267-ef27abce44d4",
   "metadata": {},
   "source": [
    "#### CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "e474c28b-4e40-4a58-a16c-e80553b2f3e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e51e88db61c94468955c7f81be60efed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating CSV from Arrow format:   0%|          | 0/160 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce36cd03b6484d93a55f872aa57b0e2f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating CSV from Arrow format:   0%|          | 0/20 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63155a0d3909424d986c0542436b7e65",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating CSV from Arrow format:   0%|          | 0/20 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for split, dataset in ds.items():\n",
    "  dataset.to_csv(f'local-allocine-{split}.csv', index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "3ad9aff2-425f-4a6d-ba13-5c3bd2081aad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d190c702e784491b840886d43e8a5f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6da321229b1a48a7bb94f89a506f0b4e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06af1808f9634d33978459eccc5de8a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28b95be32032423ca929e601a6f3058e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99df09f6684848aaae1da897f4fb3b30",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data_files = {\n",
    "  'train': 'local-allocine-train.csv',\n",
    "  'validation': 'local-allocine-validation.csv',\n",
    "  'test': 'local-allocine-test.csv',\n",
    "}\n",
    "\n",
    "csv_ds = load_dataset('csv', data_files=data_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2227c34-91ee-4a57-8243-f7905c01f302",
   "metadata": {},
   "source": [
    "#### JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "4f526e81-0a9c-4745-acc1-a813545ff8cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "473288ff2bb54df6a18579d75a5b4148",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating json from Arrow format:   0%|          | 0/160 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2ef605edf2b449388f0819517ed5849",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating json from Arrow format:   0%|          | 0/20 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6fafbb74b54485aaa29276dca2bd214",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating json from Arrow format:   0%|          | 0/20 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for split, dataset in ds.items():\n",
    "  dataset.to_json(f'local-allocine-{split}.jsonl', index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "97b97d42-236b-4f22-8dcb-6caa705dd22a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1867c1d48a24494ae7183a6b9954241",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b36733da4ad40f79440779f675d108d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "483fc209f684481789b6cccd741724c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9bed3ed914c04e349d196f9f53dd78ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e72aa25555147a0a9f9ededfba6f8d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data_files = {\n",
    "  'train': 'local-allocine-train.jsonl',\n",
    "  'validation': 'local-allocine-validation.jsonl',\n",
    "  'test': 'local-allocine-test.jsonl',\n",
    "}\n",
    "\n",
    "json_ds = load_dataset('json', data_files=data_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85cca254-44e0-4221-b53f-e7d39069742f",
   "metadata": {},
   "source": [
    "#### Parquet\n",
    "For long term (size efficient) storage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "1d9822ff-1007-4071-93be-c1c069125ee9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a15458312aa46b28da11cf04d8da4f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/160 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0d237400ccf4e4aa476782d54c025c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/20 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e030f3c79ad34362ab4b1d85475332f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/20 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for split, dataset in ds.items():\n",
    "  dataset.to_parquet(f'local-allocine-{split}.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "f215e757-dd31-4d37-a570-7fd06cd4d3a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40b2277f81cd4b8a93372550b3971f7d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "654bf79a291f44988b2527f5531be7a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8462bf70ace4712adf95d01de36c8bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d525e85da7584fdaaa658ba569b9901f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5779549c1c1a43209008f2fa22a0ee90",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data_files = {\n",
    "  'train': 'local-allocine-train.parquet',\n",
    "  'validation': 'local-allocine-validation.parquet',\n",
    "  'test': 'local-allocine-test.parquet',\n",
    "}\n",
    "\n",
    "parquet_ds = load_dataset('parquet', data_files=data_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "538aab2f-d381-4385-b16c-8ff450dca400",
   "metadata": {},
   "source": [
    "## Semantic Search with FAISS\n",
    "Using an encoder (e.g. BERT) we can convert an input sentence into an \"embedding\" (aka. a vector of integers). We can compute the similarities between different embeddings by computing the angles between them (smaller angle means more similar).\n",
    "\n",
    "The example take the github issues with at least 15 words. Concat the title body and comments into one string. Compute embeddings of it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "ae16587a-d6f6-4603-be2c-c0204b4320f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install faiss-cpu faiss-gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "195a7d2b-ad04-46f5-92ab-b68d22aecc97",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Repo card metadata block was not found. Setting CardData to empty.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>html_url</th>\n",
       "      <th>title</th>\n",
       "      <th>comments</th>\n",
       "      <th>body</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://github.com/huggingface/datasets/issues...</td>\n",
       "      <td>Protect master branch</td>\n",
       "      <td>Cool, I think we can do both :)</td>\n",
       "      <td>After accidental merge commit (91c55355b634d0d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://github.com/huggingface/datasets/issues...</td>\n",
       "      <td>Protect master branch</td>\n",
       "      <td>@lhoestq now the 2 are implemented.\\r\\n\\r\\nPle...</td>\n",
       "      <td>After accidental merge commit (91c55355b634d0d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://github.com/huggingface/datasets/issues...</td>\n",
       "      <td>Backwards compatibility broken for cached data...</td>\n",
       "      <td>Hi ! I guess the caching mechanism should have...</td>\n",
       "      <td>## Describe the bug\\r\\nAfter upgrading to data...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://github.com/huggingface/datasets/issues...</td>\n",
       "      <td>Backwards compatibility broken for cached data...</td>\n",
       "      <td>If it's easy enough to implement, then yes ple...</td>\n",
       "      <td>## Describe the bug\\r\\nAfter upgrading to data...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            html_url  \\\n",
       "0  https://github.com/huggingface/datasets/issues...   \n",
       "1  https://github.com/huggingface/datasets/issues...   \n",
       "2  https://github.com/huggingface/datasets/issues...   \n",
       "3  https://github.com/huggingface/datasets/issues...   \n",
       "\n",
       "                                               title  \\\n",
       "0                              Protect master branch   \n",
       "1                              Protect master branch   \n",
       "2  Backwards compatibility broken for cached data...   \n",
       "3  Backwards compatibility broken for cached data...   \n",
       "\n",
       "                                            comments  \\\n",
       "0                    Cool, I think we can do both :)   \n",
       "1  @lhoestq now the 2 are implemented.\\r\\n\\r\\nPle...   \n",
       "2  Hi ! I guess the caching mechanism should have...   \n",
       "3  If it's easy enough to implement, then yes ple...   \n",
       "\n",
       "                                                body  \n",
       "0  After accidental merge commit (91c55355b634d0d...  \n",
       "1  After accidental merge commit (91c55355b634d0d...  \n",
       "2  ## Describe the bug\\r\\nAfter upgrading to data...  \n",
       "3  ## Describe the bug\\r\\nAfter upgrading to data...  "
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "issues_dataset = load_dataset('lewtun/github-issues', split='train')\n",
    "issues_dataset = issues_dataset.filter(lambda x: (x['is_pull_request'] == False and len(x['comments']) > 0))\n",
    "columns = issues_dataset.column_names\n",
    "columns_to_keep = ['title', 'body', 'html_url', 'comments']\n",
    "columns_to_remove = set(columns_to_keep).symmetric_difference(columns)\n",
    "issues_dataset = issues_dataset.remove_columns(columns_to_remove)\n",
    "issues_dataset.set_format('pandas')\n",
    "df = issues_dataset[:]\n",
    "comments_df = df.explode('comments', ignore_index=True)\n",
    "comments_df.head(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "6ea174b0-38fc-4acf-ba14-522890b3c926",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52a9f5908a804a8fa272c48d7899ab90",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2964 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c27d747c09445deb3e7857e6362e198",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/2964 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3d100f5b5fa47448319e198dcb5a975",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2175 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "comments_dataset = Dataset.from_pandas(comments_df)\n",
    "# keep comments with at least 15 words\n",
    "comments_dataset = comments_dataset.map(lambda x: {'comment_length': len(x['comments'].split())})\n",
    "comments_dataset = comments_dataset.filter(lambda x: x[\"comment_length\"] > 15)\n",
    "\n",
    "def concatenate_text(examples):\n",
    "    return {\n",
    "        \"text\": examples[\"title\"]\n",
    "        + \" \\n \"\n",
    "        + examples[\"body\"]\n",
    "        + \" \\n \"\n",
    "        + examples[\"comments\"]\n",
    "    }\n",
    "comments_dataset = comments_dataset.map(concatenate_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "92aeef2d-7a82-4af9-9244-9f7a357f4481",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 768])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "\n",
    "model_ckpt = 'sentence-transformers/multi-qa-mpnet-base-dot-v1'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_ckpt)\n",
    "model = AutoModel.from_pretrained(model_ckpt)\n",
    "\n",
    "# device = torch.device('cuda')\n",
    "# windows doesn't have faiss-gpu ...\n",
    "device = torch.device('cpu')\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "def cls_pooling(model_output):\n",
    "    return model_output.last_hidden_state[:, 0]\n",
    "\n",
    "def get_embeddings(text_list):\n",
    "    encoded_input = tokenizer(text_list, padding=True, truncation=True, return_tensors='pt')\n",
    "    encoded_input = {k: v.to(device) for k, v in encoded_input.items()}\n",
    "    model_output = model(**encoded_input)\n",
    "    return cls_pooling(model_output)\n",
    "\n",
    "embedding = get_embeddings(comments_dataset['text'][0])\n",
    "embedding.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "71e8684a-ab1a-40bf-b24c-aadc8207a3e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f90bd4720bf406bbbb5587cf7e40e97",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# truncate the dataset because CPU takes 35min to compute the embeddings otherwise\n",
    "comments_dataset = comments_dataset.select(range(100))\n",
    "embeddings_dataset = comments_dataset.map(\n",
    "    lambda x: {'embeddings': get_embeddings(x[\"text\"]).detach().cpu().numpy()[0]}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "68089d4c-2492-4384-9c6e-4269c42e9bb4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4edded556aa04ca9b5ace04ab0e4190a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['html_url', 'title', 'comments', 'body', 'comment_length', 'text', 'embeddings'],\n",
       "    num_rows: 100\n",
       "})"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings_dataset.add_faiss_index(column='embeddings')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "3305761e-3a5a-4b38-9b45-2fb4de090576",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 768)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = 'How can I load a dataset offline?'\n",
    "question_embedding = get_embeddings([question]).cpu().detach().numpy()\n",
    "question_embedding.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "ffc30145-4c3f-40a1-bb1b-80f87c4fb9fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "scores, samples = embeddings_dataset.get_nearest_examples('embeddings', question_embedding, k=5)\n",
    "samples_df = pd.DataFrame.from_dict(samples)\n",
    "samples_df[\"scores\"] = scores\n",
    "samples_df.sort_values(\"scores\", ascending=False, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "a4d6fb74-b270-40c2-b96a-f47535f9db78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COMMENT: Thanks for reporting ! #2852 fixed this error\n",
      "\n",
      "We'll do a new release of `datasets` soon :)\n",
      "SCORE: 43.938785552978516\n",
      "TITLE: Cannot load linnaeus dataset\n",
      "URL: https://github.com/huggingface/datasets/issues/2821\n",
      "==================================================\n",
      "\n",
      "COMMENT: > * For the platform, we need to know the operating system of your machine. Could you please run the command `datasets-cli env` and copy-and-paste its output below?\n",
      "> * In relation with the error, you just gave us the error type and message (`TypeError: 'NoneType' object is not callable`). Could you please copy-paste the complete stack trace, so that we know exactly which part of the code threw the error?\n",
      "\n",
      "1. For the platform, here are the output:\n",
      "        - datasets` version: 1.11.0\n",
      "        - Platform: Windows-10-10.0.19041-SP0\n",
      "        - Python version: 3.7.10\n",
      "        - PyArrow version: 5.0.0\n",
      "2. For the code and error：\n",
      "     ```python\n",
      "     from datasets import load_dataset, load_metric\n",
      "     dataset = load_dataset(\"glue\", \"cola\")\n",
      "    ```\n",
      "    ```python\n",
      "    Traceback (most recent call last):\n",
      "    ....\n",
      "    ....\n",
      "    File \"my_file.py\", line 2, in <module>\n",
      "    dataset = load_dataset(\"glue\", \"cola\")\n",
      "    File \"My environments\\lib\\site-packages\\datasets\\load.py\", line 830, in load_dataset\n",
      "    **config_kwargs,\n",
      "     File \"My environments\\lib\\site-packages\\datasets\\load.py\", line 710, in load_dataset_builder\n",
      "    **config_kwargs,\n",
      "    TypeError: 'NoneType' object is not callable\n",
      "    ```\n",
      "   Thank you!\n",
      "SCORE: 43.24859619140625\n",
      "TITLE: TypeError: 'NoneType' object is not callable\n",
      "URL: https://github.com/huggingface/datasets/issues/2869\n",
      "==================================================\n",
      "\n",
      "COMMENT: > * For the platform, we need to know the operating system of your machine. Could you please run the command `datasets-cli env` and copy-and-paste its output below?\n",
      "> * In relation with the error, you just gave us the error type and message (`TypeError: 'NoneType' object is not callable`). Could you please copy-paste the complete stack trace, so that we know exactly which part of the code threw the error?\n",
      "\n",
      "1. For the platform, here are the output:\n",
      "        - datasets` version: 1.11.0\n",
      "        - Platform: Windows-10-10.0.19041-SP0\n",
      "        - Python version: 3.7.10\n",
      "        - PyArrow version: 5.0.0\n",
      "2. For the code and error：\n",
      "     ```python\n",
      "     from datasets import load_dataset, load_metric\n",
      "     dataset = load_dataset(\"glue\", \"cola\")\n",
      "    ```\n",
      "    ```python\n",
      "    Traceback (most recent call last):\n",
      "    ....\n",
      "    ....\n",
      "    File \"my_file.py\", line 2, in <module>\n",
      "    dataset = load_dataset(\"glue\", \"cola\")\n",
      "    File \"My environments\\lib\\site-packages\\datasets\\load.py\", line 830, in load_dataset\n",
      "    **config_kwargs,\n",
      "     File \"My environments\\lib\\site-packages\\datasets\\load.py\", line 710, in load_dataset_builder\n",
      "    **config_kwargs,\n",
      "    TypeError: 'NoneType' object is not callable\n",
      "    ```\n",
      "   Thank you!\n",
      "SCORE: 43.24859619140625\n",
      "TITLE: TypeError: 'NoneType' object is not callable\n",
      "URL: https://github.com/huggingface/datasets/issues/2869\n",
      "==================================================\n",
      "\n",
      "COMMENT: One naive question: do you have internet access from the machine where you execute the code?\n",
      "SCORE: 42.89968490600586\n",
      "TITLE: TypeError: 'NoneType' object is not callable\n",
      "URL: https://github.com/huggingface/datasets/issues/2869\n",
      "==================================================\n",
      "\n",
      "COMMENT: One naive question: do you have internet access from the machine where you execute the code?\n",
      "SCORE: 42.89968490600586\n",
      "TITLE: TypeError: 'NoneType' object is not callable\n",
      "URL: https://github.com/huggingface/datasets/issues/2869\n",
      "==================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for _, row in samples_df.iterrows():\n",
    "    print(f\"COMMENT: {row.comments}\")\n",
    "    print(f\"SCORE: {row.scores}\")\n",
    "    print(f\"TITLE: {row.title}\")\n",
    "    print(f\"URL: {row.html_url}\")\n",
    "    print(\"=\" * 50)\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_hf_nlp",
   "language": "python",
   "name": "venv_hf_nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
